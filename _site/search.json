[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{essi2022,\n  author = {Essi},\n  editor = {},\n  title = {Post {With} {Code}},\n  date = {2022-09-14},\n  url = {https://ealizadeh.com/blog/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. 2022. ‚ÄúPost With Code.‚Äù September 14, 2022. https://ealizadeh.com/blog/post-with-code."
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{o'malley2022,\n  author = {Tristan O‚ÄôMalley},\n  editor = {},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2022-09-11},\n  url = {https://ealizadeh.com/blog/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTristan O‚ÄôMalley. 2022. ‚ÄúWelcome To My Blog.‚Äù September 11,\n2022. https://ealizadeh.com/blog/welcome."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "K-Means\n\n\nKNN\n\n\nMachine Learnin\n\n\n\n\n\n\n\nEssi\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomation\n\n\nGitHub Actions\n\n\nCron\n\n\n\n\n\n\n\nEssi\n\n\nJan 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nVisualization\n\n\nPandas\n\n\n\n\n\n\n\nEssi\n\n\nDec 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL\n\n\nData Science\n\n\nDatabase\n\n\nETL\n\n\nTutorial\n\n\n\n\n\n\n\nEssi\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "I‚Äôm an engineer and a data scientist.\n~ In Permanent Beta: Learning, Improving, Evolving ~"
  },
  {
    "objectID": "blog/automate-github-actions-cron/index.html",
    "href": "blog/automate-github-actions-cron/index.html",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{essi2022,\n  author = {Essi},\n  editor = {},\n  title = {Automate {Your} {Workflow} with {GitHub} {Actions} and\n    {Cron}},\n  date = {2022-01-20},\n  url = {https://ealizadeh.com/blog/automate-github-actions-cron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. 2022. ‚ÄúAutomate Your Workflow with GitHub Actions and\nCron.‚Äù January 20, 2022. https://ealizadeh.com/blog/automate-github-actions-cron."
  },
  {
    "objectID": "blog/automate-github-actions-cron/index.html#header-2",
    "href": "blog/automate-github-actions-cron/index.html#header-2",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Header 2",
    "text": "Header 2\nThis is a post with executable code.\n\nHeader 3\nThis is a po"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html",
    "href": "blog/automate-workflow-github-cron/index.html",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†Towards Data Science blog."
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#considerations-before-automation",
    "href": "blog/automate-workflow-github-cron/index.html#considerations-before-automation",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Considerations before automation",
    "text": "Considerations before automation\nYour script will most likely be different. However, here the following tips may help you get started:\nFirst, run the script on your system. Once you‚Äôre happy with the result and you want to automate the workflow, then use the instructions below to setup a scheduled workflow using GitHub Actions.\nWhen developing an automation task, it is always good to think about how to run it starting from a fresh OS! It is as if you have a new system and you try to run your script there. A few question to ask yourself:\n\nWhere should I start?\nWhat are software/libraries I need to install before running the script?\nWhere can I find the script I want to run?\nDo I need to pass some environment variables or sensitive information like passwords?\n\nHow should I pass sensitive information like passwords or tokens?\n\n\nI will answer above questions for my workflow. Hopefully this will give you enough information to automate your task!"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#cron-job-examples",
    "href": "blog/automate-workflow-github-cron/index.html#cron-job-examples",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Cron job examples",
    "text": "Cron job examples\nBelow examples covers different aspects of a cron syntax and all valid characters (* , - /).\n\nYou can specify your schedule by choosing a valid number for each part. * means ‚Äúevery‚Äù (* * * * * means at every minute on every hour of every day of the month in every month at every day of the week üôÇ). Another example is 30 13 1 * * meaning at 13:30 on day 1 of the month.\nYou can have multiple parameters for a given section by using the value list separator ,. For instance, * * * * 0,3 means every minute only on Sunday and Wednesday.\nYou can have step values by using /. For instance, /10 * * * * means every 10 minutes.\nYou can have a range of values by using dash -. For instance, 4-5 1-10 1 * means every minute between 04:00 - 05:59 AM between day 1 and day 10 of January.\n\nAnd of course, you can have a combination of above options. For example, */30 1-5 * 1,6 0,1 means every 30 minutes between 01:00-05:59 AM only on Sunday and Monday in January and June.\nCheck¬†crontab or crontab guru¬†to come up with the cron syntax for your schedule."
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#where-should-we-start-from",
    "href": "blog/automate-workflow-github-cron/index.html#where-should-we-start-from",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Where should we start from?",
    "text": "Where should we start from?\nWe can start from a fresh Ubuntu system. So, we have the section below the jobs specifying runs-on: ubuntu-latest that will configures the job to run on a fresh virtual machine containing the latest version of an Ubuntu Linux.\nNext step is to clone the current repo. You can achieve this by using uses keyword allowing us to use any action from the GitHub Actions Marketplace . We can use the master branch of actions/checkout here (you can also specify the version like actions/checkout@v2).\n- name: üçΩÔ∏è Checkout the repo\n  uses: actions/checkout@master\n  with:\n    fetch-depth: 1"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#which-softwarelibraries-we-must-install",
    "href": "blog/automate-workflow-github-cron/index.html#which-softwarelibraries-we-must-install",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Which software/libraries we must install?",
    "text": "Which software/libraries we must install?\nThis step is only necessary if you have to install a library. In my case, I have to first install Python 3.8. This can be achieved by using the actions/setup-python@v2 GitHub Action. Afterwards, we want to install the python package. We can install the Zotero2Readwise¬†package by running pip install zotero2readwise. However, in order to execute a command on the runner, we have to use the run keyword.\n- name: üêç Set up Python 3.8\n  uses: actions/setup-python@v2\n  with:\n    python-version: '3.8'\n\n- name: üíø Install Zotero2Readwise Python package\n  run: pip install zotero2readwise"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#where-can-i-find-the-script-i-want-to-run",
    "href": "blog/automate-workflow-github-cron/index.html#where-can-i-find-the-script-i-want-to-run",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Where can I find the script I want to run?",
    "text": "Where can I find the script I want to run?\nIf the script you are trying to run lives in the same repository, you can just skip this step. But here, since the Python script I want to run lives in another GitHub repository, I have to download the script using the curl Linux command.\n- name: üì• Download the Python script needed for automation\n  run:  curl https://raw.githubusercontent.com/e-alizadeh/Zotero2Readwise/master/zotero2readwise/run.py -o run.py"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#run-the-script",
    "href": "blog/automate-workflow-github-cron/index.html#run-the-script",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Run the script",
    "text": "Run the script\nNow that we have set up our environment, we can run the script as mentioned earlier in the Requirements section.\nBut one last point is that since we need to pass some sensitive information (like tokens), we can achieve that by passing the secrets to Settings ‚Üí Secrets ‚Üí New repository secret.\n\n\n\n\n\nHow to pass secrets to the environment of a GitHub repository\n\n\nFigure¬†1: ?(caption)\n\n\nThese secrets will then be available using the following syntax: ${{ secrets.YOUR_SECRET_NAME }} in your YAML file.\nFor more information about handling variables and secrets, you can check the following two pages on the GitHub Docs about Environment variables and Encrypted secrets.\nNow that we have added our secrets, we can run the script as following:\n- name: üöÄ Run Automation\n  run: python run.py ${{ secrets.READWISE_TOKEN }} ${{ secrets.ZOTERO_KEY }} ${{ secrets.ZOTERO_ID }}"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#putting-everything-together",
    "href": "blog/automate-workflow-github-cron/index.html#putting-everything-together",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Putting everything together",
    "text": "Putting everything together\nThe file containing all steps above is shown below. The file lives on GitHub.\nname: Zotero to Readwise Automation\n\non:\n  push:\n    branches:\n      - master\n  schedule:\n    - cron: \"0 3 * * 1,3,5\" # Runs at 03:00 AM (UTC) every Monday, Wednesday, and Friday (Check https://crontab.guru/)\n\njobs:\n  zotero-to-readwise-automation:\n    runs-on: ubuntu-latest\n    steps:\n      - name: üçΩÔ∏è Checkout the repo\n        uses: actions/checkout@master\n        with:\n          fetch-depth: 1\n\n      - name: üêç Set up Python 3.8\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n\n      - name: üíø Install Zotero2Readwise Python package\n        run: pip install zotero2readwise\n\n      - name: üì• Download the Python script needed for automation\n        run:  curl https://raw.githubusercontent.com/e-alizadeh/Zotero2Readwise/master/zotero2readwise/run.py -o run.py\n\n      - name: üöÄ Run Automation\n        run: python run.py ${{ secrets.READWISE_TOKEN }} ${{ secrets.ZOTERO_KEY }} ${{ secrets.ZOTERO_ID }}\n\n\n\nA screenshot of the GitHub Actions showing how the workflow is run on a schedule or via a push to the master branch."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html",
    "href": "blog/pandas-tutor-tool/index.html",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†Towards Data Science blog."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#pandastutor-creators",
    "href": "blog/pandas-tutor-tool/index.html#pandastutor-creators",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "PandasTutor Creators",
    "text": "PandasTutor Creators\nPandas Tutor was created by Sam Lau¬†and¬†Philip Guo at UC San Diego. This tool is mainly developed for teaching purposes as its creator stated here. This explains some of the limitations this tool have (I will cover some of those limitations later in the post).\nA similar tool called Tidy Data Tutor but for R users is created by Sean Kross¬†and¬†Philip Guo."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#dataset",
    "href": "blog/pandas-tutor-tool/index.html#dataset",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Dataset",
    "text": "Dataset\nLet‚Äôs use the Heart Failure Prediction Dataset Kaggle Dataset (available here). The data is available under¬†Open Database (ODbl) License¬†allowing¬†‚Äúusers to freely share, modify, and use this Database while maintaining this same freedom for others.‚Äù Since Pandas Tutor only works with small data, I will take the first 50 rows of hearts data)."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#code",
    "href": "blog/pandas-tutor-tool/index.html#code",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Code",
    "text": "Code\nBelow is the code used for the visualization in this post. You may notice that the CSV data is encoded here which is a current limitation of this tool.\nimport pandas as pd\nimport io\n\ncsv = '''\nAge,Sex,ChestPainType,RestingBP,Cholesterol,FastingBS,RestingECG,MaxHR,ExerciseAngina,Oldpeak,ST_Slope,HeartDisease\n40,M,ATA,140,289,0,Normal,172,N,0,Up,0\n49,F,NAP,160,180,0,Normal,156,N,1,Flat,1\n37,M,ATA,130,283,0,ST,98,N,0,Up,0\n48,F,ASY,138,214,0,Normal,108,Y,1.5,Flat,1\n54,M,NAP,150,195,0,Normal,122,N,0,Up,0\n39,M,NAP,120,339,0,Normal,170,N,0,Up,0\n45,F,ATA,130,237,0,Normal,170,N,0,Up,0\n54,M,ATA,110,208,0,Normal,142,N,0,Up,0\n37,M,ASY,140,207,0,Normal,130,Y,1.5,Flat,1\n48,F,ATA,120,284,0,Normal,120,N,0,Up,0\n37,F,NAP,130,211,0,Normal,142,N,0,Up,0\n58,M,ATA,136,164,0,ST,99,Y,2,Flat,1\n39,M,ATA,120,204,0,Normal,145,N,0,Up,0\n49,M,ASY,140,234,0,Normal,140,Y,1,Flat,1\n42,F,NAP,115,211,0,ST,137,N,0,Up,0\n54,F,ATA,120,273,0,Normal,150,N,1.5,Flat,0\n38,M,ASY,110,196,0,Normal,166,N,0,Flat,1\n43,F,ATA,120,201,0,Normal,165,N,0,Up,0\n60,M,ASY,100,248,0,Normal,125,N,1,Flat,1\n36,M,ATA,120,267,0,Normal,160,N,3,Flat,1\n43,F,TA,100,223,0,Normal,142,N,0,Up,0\n44,M,ATA,120,184,0,Normal,142,N,1,Flat,0\n49,F,ATA,124,201,0,Normal,164,N,0,Up,0\n44,M,ATA,150,288,0,Normal,150,Y,3,Flat,1\n40,M,NAP,130,215,0,Normal,138,N,0,Up,0\n36,M,NAP,130,209,0,Normal,178,N,0,Up,0\n53,M,ASY,124,260,0,ST,112,Y,3,Flat,0\n52,M,ATA,120,284,0,Normal,118,N,0,Up,0\n53,F,ATA,113,468,0,Normal,127,N,0,Up,0\n51,M,ATA,125,188,0,Normal,145,N,0,Up,0\n53,M,NAP,145,518,0,Normal,130,N,0,Flat,1\n56,M,NAP,130,167,0,Normal,114,N,0,Up,0\n54,M,ASY,125,224,0,Normal,122,N,2,Flat,1\n41,M,ASY,130,172,0,ST,130,N,2,Flat,1\n43,F,ATA,150,186,0,Normal,154,N,0,Up,0\n32,M,ATA,125,254,0,Normal,155,N,0,Up,0\n65,M,ASY,140,306,1,Normal,87,Y,1.5,Flat,1\n41,F,ATA,110,250,0,ST,142,N,0,Up,0\n48,F,ATA,120,177,1,ST,148,N,0,Up,0\n48,F,ASY,150,227,0,Normal,130,Y,1,Flat,0\n54,F,ATA,150,230,0,Normal,130,N,0,Up,0\n54,F,NAP,130,294,0,ST,100,Y,0,Flat,1\n35,M,ATA,150,264,0,Normal,168,N,0,Up,0\n52,M,NAP,140,259,0,ST,170,N,0,Up,0\n43,M,ASY,120,175,0,Normal,120,Y,1,Flat,1\n59,M,NAP,130,318,0,Normal,120,Y,1,Flat,0\n37,M,ASY,120,223,0,Normal,168,N,0,Up,0\n50,M,ATA,140,216,0,Normal,170,N,0,Up,0\n36,M,NAP,112,340,0,Normal,184,N,1,Flat,0\n41,M,ASY,110,289,0,Normal,170,N,0,Flat,1\n'''\n\ndf_hearts = pd.read_csv(io.StringIO(csv))\ndf_hearts = df_hearts[\n    [\"Age\", \"Sex\", \"RestingBP\", \"ChestPainType\", \"Cholesterol\", \"HeartDisease\"]\n]\n\n(df_hearts.sort_values(\"Age\")\n.groupby([\"Sex\", \"HeartDisease\"])\n.agg({\"RestingBP\": [\"mean\", \"std\"], \n      \"Cholesterol\": [\"mean\", \"std\"],\n      \"Sex\": [\"count\"]\n      })\n)\nSo our transformations is only the last few lines\n(df_hearts.sort_values(\"Age\")\n.groupby([\"Sex\", \"HeartDisease\"])\n.agg({\"RestingBP\": [\"mean\", \"std\"], \n      \"Cholesterol\": [\"mean\", \"std\"],\n      \"Sex\": [\"count\"]\n      })\n)"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#results",
    "href": "blog/pandas-tutor-tool/index.html#results",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Results",
    "text": "Results\n\nStep 1: Sorting the DataFrame\n\n\n\nVisualization of the sort_values() result (steps 1) (generated using PandasTutor)\n\n\nVisualization of the sort_values() result (steps 1) (generated using PandasTutor)\n\n\nStep 2: Visualize Pandas Groupby operation\nAfter sorting the results in Step 1 and visualizing it, we can visualize the groupby() operation\n\n\n\nVisualization of the groupby() result (steps 1 and 2) (generated using PandasTutor)\n\n\n\n\nStep 3: Calculate different aggregations on multiple columns\nHere, I will be calculating the mean and standard deviation of two columns ‚ÄúRestingBP‚Äù and ‚ÄúCholesterol‚Äù and also provide a count for each group (here I‚Äôm using the ‚ÄúSex‚Äù column to get that information.)\n\n\n\nVisualization of the final result that is the aggregation (steps 1 - 3) (generated using PandasTutor)\n\n\nVisualization of the final result that is the aggregation (steps 1 - 3) (generated using PandasTutor)\n\n\nInteresting sharing feature\nPandas Tutor also provides you with a shareable URL that even includes the CSV data used in the transformation. For instance, you can check my transformation code and results here or via below link!\nhttps://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20io%0A%0Acsv%20%3D%20'''%0AAge,Sex,ChestPainType,RestingBP,Cholesterol,FastingBS,RestingECG,MaxHR,ExerciseAngina,Oldpeak,ST_Slope,HeartDisease%0A40,M,ATA,140,289,0,Normal,172,N,0,Up,0%0A49,F,NAP,160,180,0,Normal,156,N,1,Flat,1%0A37,M,ATA,130,283,0,ST,98,N,0,Up,0%0A48,F,ASY,138,214,0,Normal,108,Y,1.5,Flat,1%0A54,M,NAP,150,195,0,Normal,122,N,0,Up,0%0A39,M,NAP,120,339,0,Normal,170,N,0,Up,0%0A45,F,ATA,130,237,0,Normal,170,N,0,Up,0%0A54,M,ATA,110,208,0,Normal,142,N,0,Up,0%0A37,M,ASY,140,207,0,Normal,130,Y,1.5,Flat,1%0A48,F,ATA,120,284,0,Normal,120,N,0,Up,0%0A37,F,NAP,130,211,0,Normal,142,N,0,Up,0%0A58,M,ATA,136,164,0,ST,99,Y,2,Flat,1%0A39,M,ATA,120,204,0,Normal,145,N,0,Up,0%0A49,M,ASY,140,234,0,Normal,140,Y,1,Flat,1%0A42,F,NAP,115,211,0,ST,137,N,0,Up,0%0A54,F,ATA,120,273,0,Normal,150,N,1.5,Flat,0%0A38,M,ASY,110,196,0,Normal,166,N,0,Flat,1%0A43,F,ATA,120,201,0,Normal,165,N,0,Up,0%0A60,M,ASY,100,248,0,Normal,125,N,1,Flat,1%0A36,M,ATA,120,267,0,Normal,160,N,3,Flat,1%0A43,F,TA,100,223,0,Normal,142,N,0,Up,0%0A44,M,ATA,120,184,0,Normal,142,N,1,Flat,0%0A49,F,ATA,124,201,0,Normal,164,N,0,Up,0%0A44,M,ATA,150,288,0,Normal,150,Y,3,Flat,1%0A40,M,NAP,130,215,0,Normal,138,N,0,Up,0%0A36,M,NAP,130,209,0,Normal,178,N,0,Up,0%0A53,M,ASY,124,260,0,ST,112,Y,3,Flat,0%0A52,M,ATA,120,284,0,Normal,118,N,0,Up,0%0A53,F,ATA,113,468,0,Normal,127,N,0,Up,0%0A51,M,ATA,125,188,0,Normal,145,N,0,Up,0%0A53,M,NAP,145,518,0,Normal,130,N,0,Flat,1%0A56,M,NAP,130,167,0,Normal,114,N,0,Up,0%0A54,M,ASY,125,224,0,Normal,122,N,2,Flat,1%0A41,M,ASY,130,172,0,ST,130,N,2,Flat,1%0A43,F,ATA,150,186,0,Normal,154,N,0,Up,0%0A32,M,ATA,125,254,0,Normal,155,N,0,Up,0%0A65,M,ASY,140,306,1,Normal,87,Y,1.5,Flat,1%0A41,F,ATA,110,250,0,ST,142,N,0,Up,0%0A48,F,ATA,120,177,1,ST,148,N,0,Up,0%0A48,F,ASY,150,227,0,Normal,130,Y,1,Flat,0%0A54,F,ATA,150,230,0,Normal,130,N,0,Up,0%0A54,F,NAP,130,294,0,ST,100,Y,0,Flat,1%0A35,M,ATA,150,264,0,Normal,168,N,0,Up,0%0A52,M,NAP,140,259,0,ST,170,N,0,Up,0%0A43,M,ASY,120,175,0,Normal,120,Y,1,Flat,1%0A59,M,NAP,130,318,0,Normal,120,Y,1,Flat,0%0A37,M,ASY,120,223,0,Normal,168,N,0,Up,0%0A50,M,ATA,140,216,0,Normal,170,N,0,Up,0%0A36,M,NAP,112,340,0,Normal,184,N,1,Flat,0%0A41,M,ASY,110,289,0,Normal,170,N,0,Flat,1%0A'''%0A%0Adf_hearts%20%3D%20pd.read_csv%28io.StringIO%28csv%29%29%0Adf_hearts%20%3D%20df_hearts%5B%0A%20%20%20%20%5B%22Age%22,%20%22Sex%22,%20%22RestingBP%22,%20%22ChestPainType%22,%20%22Cholesterol%22,%20%22HeartDisease%22%5D%0A%5D%0A%0A%28df_hearts.sort_values%28%22Age%22%29%0A.groupby%28%5B%22Sex%22,%20%22HeartDisease%22%5D%29%0A.agg%28%7B%22RestingBP%22%3A%20%5B%22mean%22,%20%22std%22%5D,%20%0A%20%20%20%20%20%20%22Cholesterol%22%3A%20%5B%22mean%22,%20%22std%22%5D,%0A%20%20%20%20%20%20%22Sex%22%3A%20%5B%22count%22%5D%0A%20%20%20%20%20%20%7D%29%0A%29&d=2021-12-08&lang=py&v=v1"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#pros",
    "href": "blog/pandas-tutor-tool/index.html#pros",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Pros:",
    "text": "Pros:\n\nStep-by-step visualization\nInteractive plots (you can track the data rows before and after the transformation)\nShareable URL"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#cons-current-limitations",
    "href": "blog/pandas-tutor-tool/index.html#cons-current-limitations",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Cons (current limitations):",
    "text": "Cons (current limitations):\n\nOnly works for small codes (The code should be 5000bytes). Since the data is also encoded and not read from a file, hence, you can only visualize small datasets.\nAs stated in the previous step, you have to encode the data along with the code as reading from external resources (files or links) are not supported.\nLimited Pandas‚Äô methods support.\nYou can visualize the Pandas expression only on the last line. You may have to pipe multiple steps together or run the visualizations separately.\n\nFor a complete list of unsupported features or other FAQ, you can check here."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html",
    "href": "blog/dbt-tutorial/index.html",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†KDnuggets."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#signup",
    "href": "blog/dbt-tutorial/index.html#signup",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Signup",
    "text": "Signup\nYou can sign up at getdbt.com. The free plan is a great plan for small projects and testing."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#database-with-populated-data",
    "href": "blog/dbt-tutorial/index.html#database-with-populated-data",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Database with populated data",
    "text": "Database with populated data\nYou can check my post on how to deploy a free PostgreSQL database on Heroku. The post provides step-by-step instructions on how to do it.\nYou can also check the data ingestion script in the GitHub repo accompanying this article.\ne-alizadeh/sample_dbt_project\nFollowing the above, we have generated two tables in a PostgreSQL database that we are going to use in this post. There are two tables in the database, namely covid_latest and population_prosperity. You can find the ingestion script on the GitHub repo for this post."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-cli-installation",
    "href": "blog/dbt-tutorial/index.html#dbt-cli-installation",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt CLI Installation",
    "text": "dbt CLI Installation\nYou can install the dbt command-line interface (CLI) by following the instructions on the following dbt documentation page.\nInstallation | dbt Docs"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#how-to-use-dbt",
    "href": "blog/dbt-tutorial/index.html#how-to-use-dbt",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "How to use dbt?",
    "text": "How to use dbt?\nA dbt project is a directory containing .sql and .yml files. The minimum required files are:\n\nA project file named dbt_project.yml: This file contains configurations of a dbt project.\nModel(s) (.sql files): A model in dbt is simply a single .sql file containing a single select statement.\n\nEvery dbt project needs a dbt_project.yml file ‚Äî this is how dbt knows a directory is a dbt project. It also contains important information that tells dbt how to operate on your project.\nYou can find more information about dbt projects here.\n\n\n\n\n\n\nNote\n\n\n\nüí° A dbt model is basically a .sql file with a SELECT statement."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-commands",
    "href": "blog/dbt-tutorial/index.html#dbt-commands",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt Commands",
    "text": "dbt Commands\ndbt commands start with dbt and can be executed using one of the following ways:\n\ndbt Cloud (the command section at the bottom of the dbt Cloud dashboard),\ndbt CLI\n\nSome commands can only be used in dbt CLI like dbt init. Some dbt commands we will use in this post are\n\ndbt init (only in dbt CLI)\ndbt run\ndbt test\ndbt docs generate"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-1-initialize-a-dbt-project-sample-files-using-dbt-cli",
    "href": "blog/dbt-tutorial/index.html#step-1-initialize-a-dbt-project-sample-files-using-dbt-cli",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 1: Initialize a dbt project (sample files) using dbt CLI",
    "text": "Step 1: Initialize a dbt project (sample files) using dbt CLI\nYou can use [dbt init](https://docs.getdbt.com/reference/commands/init) to generate sample files/folders. In particular, dbt init project_name will create the following:\n\na¬†~/.dbt/profiles.yml¬†file if one does not already exist\na new folder called¬†[project_name]\ndirectories and sample files necessary to get started with dbt\n\n\n\n\n\n\n\nWarning\n\n\n\nSince dbt init generates a directory namedproject_name, and in order to avoid any conflict, you should not have any existing folder with an identical name.\n\n\n\n\n\ndbt init \n\n\nThe result is a directory with the following sample files.\nsample_dbt_project\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ analysis\n‚îú‚îÄ‚îÄ data\n‚îú‚îÄ‚îÄ dbt_project.yml\n‚îú‚îÄ‚îÄ macros\n‚îú‚îÄ‚îÄ models\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ example\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ my_first_dbt_model.sql\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ my_second_dbt_model.sql\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ schema.yml\n‚îú‚îÄ‚îÄ snapshots\n‚îî‚îÄ‚îÄ tests\nFor this post, we will just consider the minimum files and remove the extra stuff.\nsample_dbt_project\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ dbt_project.yml\n‚îî‚îÄ‚îÄ models\n ¬†¬† ‚îú‚îÄ‚îÄ my_first_dbt_model.sql\n    ‚îú‚îÄ‚îÄ my_second_dbt_model.sql\n¬†   ‚îî‚îÄ‚îÄ schema.yml"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-2-set-up-a-git-repository",
    "href": "blog/dbt-tutorial/index.html#step-2-set-up-a-git-repository",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 2: Set Up a Git Repository",
    "text": "Step 2: Set Up a Git Repository\nYou can use an existing repo, as specified during the setup. You can configure the repositories by following the dbt documentation here.\n\nOr, if you want to create a new repo‚Ä¶\nyou can create a new repository from inside the created directory. You can do that as below\ngit init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origing <repo_url>\ngit push -u origin master"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-3-set-up-a-new-project-on-dbt-cloud-dashboard",
    "href": "blog/dbt-tutorial/index.html#step-3-set-up-a-new-project-on-dbt-cloud-dashboard",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 3: Set Up a New Project on dbt Cloud Dashboard",
    "text": "Step 3: Set Up a New Project on dbt Cloud Dashboard\nIn the previous step, we created a sample dbt project containing sample models and configurations. Now, we want to create a new project and connect our database and repository on the dbt Cloud dashboard.\nBefore we continue, you should have\n\nsome data already available in a database,\na repository with the files generated at the previous step\n\nYou can follow the steps below to set up a new project in dbt Cloud (keep in mind this step is different than the previous step in that we only generated some sample files).\n\n\nSet up a new dbt project on dbt Cloud\nThe dbt_project.yml file for our project is shown below (you can find the complete version in the GitHub repo to this post).\nname: 'my_new_project'\nversion: '1.0.0'\nconfig-version: 2\n\nvars:\n  selected_country: USA\n    selected_year: 2019\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'default'\n\n# There are other stuff that are generated automatically when you run `dbt init`"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-models",
    "href": "blog/dbt-tutorial/index.html#dbt-models",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt models",
    "text": "dbt models\nLet‚Äôs create simple dbt models that retrieve few columns of the tables.\nselect \"iso_code\", \"total_cases\", \"new_cases\" from covid_latest\nselect \"code\", \"year\", \"continent\", \"total_population\" from population_prosperity\n\n\n\n\n\n\nWarning\n\n\n\nThe dbt model name is the filename of the sql file in the models directory. The model name may differ from the table name in the database. For instance, in above, the dbt model population is the result of a SELECT statement on population_prosperity table in the database.\n\n\n\nRun models\nYou can run all models in your dbt project by executing dbt run. A sample dbt run output is shown below. You can see a summary or detailed log of running all dbt models. This helps a lot to debug any issue you may have in the queries. For instance, you can see a failed model that throws a Postgres error.\n\n\n\nDetailed log of failed jinja_and_variable_usage dbt model"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#jinja-macros",
    "href": "blog/dbt-tutorial/index.html#jinja-macros",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Jinja & Macros",
    "text": "Jinja & Macros\ndbt uses Jinja templating language, making a dbt project an ideal programming environment for SQL. With Jinja, you can do transformations that are not normally possible in SQL, like using environment variables, or macros ‚Äî abstract snippets of SQL, which is analogous to functions in most programming languages. Whenever you see a {{ ... }}, you‚Äôre already using Jinja. For more information about Jinja and additional Jinja-style functions defined, you can check dbt documentation.\nLater in this post, we will cover custom macros defined by dbt."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#using-variables",
    "href": "blog/dbt-tutorial/index.html#using-variables",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Using Variables",
    "text": "Using Variables\n\nDefine a variable\nYou can define your variables under the vars section in your dbt_project.yml. For instance, let‚Äôs define a variable called selected_country whose default value is USA and another one called selected_year whose default value is 2019.\nname: 'my_new_project'\nversion: '1.0.0'\nconfig-version: 2\n\nvars:\n  selected_country: USA\n    selected_year: 2019\n\n\nUse a Variable\nYou can use variables in your dbt models via [var()](https://docs.getdbt.com/reference/dbt-jinja-functions/var) Jinja function ({{ var(\"var_key_name\") }} ."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#macros",
    "href": "blog/dbt-tutorial/index.html#macros",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Macros",
    "text": "Macros\nThere are many useful transformations and useful macros in dbt_utils that can be used in your project. For a list of all available macros, you can check their GitHub repo.\nNow, let‚Äôs add dbt_utils to our project and install it by following the below steps:\n\nAdd dbt_utils macro to your¬†packages.yml¬†file, as follows:\n\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 0.6.6\n\nRun¬†dbt deps¬†to install the package.\n\n\n\n\nInstall packages using dbt deps"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#complex-dbt-models",
    "href": "blog/dbt-tutorial/index.html#complex-dbt-models",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Complex dbt models",
    "text": "Complex dbt models\nThe models (selects) are usually stacked on top of one another. For building more complex models, you will have to use [ref()](https://docs.getdbt.com/reference/dbt-jinja-functions/ref) macro. ref() is the most important function in dbt as it allows you to refer to other models. For instance, you may have a model (aka SELECT query) that does multiple stuff, and you don‚Äôt want to use it in other models. It will be difficult to build a complex model without using macros introduced earlier.\n\ndbt model using ref() and global variables\nWe can build more complex models using the two dbt models defined earlier in the post. For instance, let‚Äôs create a new dbt model that joins the above two tables on the country code and then filters based on selected country and year.\nselect *\nfrom {{ref('population')}} \ninner join {{ref('covid19_latest_stats')}} \non {{ref('population')}}.code = {{ref('covid19_latest_stats')}}.iso_code \nwhere code='{{ var(\"selected_country\") }}' AND year='{{ var(\"selected_year\") }}'\nFew points about the query above:\n\n{{ref('dbt_model_name')}}is used to refer to dbt models available in the project.\nYou can get a column from the model like {{ref('dbt_model_name')}}.column_name.\nYou can use variables defined in dbt_project.yml file by {{var(\"variable_name)}}.\n\nThe abbove code snippet joins the data from population and covid19_latest_stats models on the country code and filters them based on the selected_country=USA and selected_year=2019. The output of the model is shown below.\n\n\n\nThe output of the jinja_and_variable_usage dbt model\n\n\nYou can also see the compiled SQL code snippet by clicking on compile sql button. This is very useful particularly if you want to run the query outside the dbt tool.\n\n\n\nCompiled SQL code for jinja_and_variable_usage dbt model\n\n\n\n\ndbt model using dbt_utils package and macros\ndbt_utils package contains macros (aka functions) you can use in your dbt projects. A list of all macros is available on dbt_utils‚Äô GitHub page.\nLet‚Äôs use dbt_utils [pivot()](https://github.com/dbt-labs/dbt-utils/#pivot-source) and [get_column_values()](https://github.com/dbt-labs/dbt-utils/#get_column_values-source) macros in a dbt model as below:\nselect\n  continent,\n  {{ dbt_utils.pivot(\n      \"population.year\",\n      dbt_utils.get_column_values(ref('population'), \"year\")\n  ) }}\nfrom {{ ref('population') }}\ngroup by continent\nThe above dbt model will compile to the following SQL query in dbt.\nselect\n  continent,\n    sum(case when population.year = '2015' then 1 else 0 end) as \"2015\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2017\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2016\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2018\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2019\"\nfrom \"d15em1n30ihttu\".\"dbt_ealizadeh\".\"population\"\ngroup by continent\nlimit 500\n/* limit added automatically by dbt cloud */"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#database-administration-using-hooks-operations",
    "href": "blog/dbt-tutorial/index.html#database-administration-using-hooks-operations",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Database administration using Hooks & Operations",
    "text": "Database administration using Hooks & Operations\nThere are database management tasks that require running additional SQL queries, such as:\n\nCreate user-defined functions\nGrant privileges on a table\nand many more\n\ndbt has two interfaces (hooks and operations) for executing these tasks and importantly version control them. Hooks and operations are briefly introduced here. For more info, you can check dbt documentation.\n\nHooks\nHooks are simply SQL snippets that are executed at different times. Hooks are defined in the dbt_project.yml file. Different hooks are:\n\npre-hook: executed before a model is built\npost-hook: executed after a model is built\non-run-start: executed at the start of dbt run\non-run-end: executed at the end of dbt run\n\n\n\nOperations\nOperations are a convenient way to invoke a macro without running a model. Operations are triggered using [dbt run-operation](https://docs.getdbt.com/reference/commands/run-operation) command.\nNote that, unlike hooks, you need to explicitly execute the SQL in a dbt operation."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html",
    "href": "blog/knn-and-kmeans/index.html",
    "title": "What K is in KNN and K-Means",
    "section": "",
    "text": "In this post, we will go over two popular machine learning algorithms: K-Nearest Neighbors (aka KNN) and K-Means, and what K stands for in each algorithm. An overview of both popular ML techniques (including a visual illustration) will be provided.\nBy the end of this post, we will be able to answer the following questions:\n\nWhat‚Äôs the difference between KNN and K-Means?\nWhat does K mean in KNN and K-Means?\nWhat is a nonparametric model?\nWhat is a lazy learner model?\nWhat is within-cluster sum of squares, WCSS (aka intracluster inertia/distance, within-cluster variance)?\nHow to determine the best value K in K-Means?\nWhat are pros and cons of KNN?\nWhat are pros and cons of K-Means?\n\n\n\n\n\n\n\nNote\n\n\n\nüëâ The goal of this post is not to compare KNN and K-Means as each one addresses a different problem. Hence, comparing them is like comparing apples to oranges."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#an-illustration-of-k-nn",
    "href": "blog/knn-and-kmeans/index.html#an-illustration-of-k-nn",
    "title": "What K is in KNN and K-Means",
    "section": "An Illustration of K-NN",
    "text": "An Illustration of K-NN\nAs I mentioned earlier, KNN is a supervised learning technique, so we should have a labeled dataset. Let‚Äôs say we have two classes as can be seen in below image: Class A (blue points) and Class B (green points). A new data point (red) is given to us and we want to predict whether the new point belongs to Class A or Class B.\nLet‚Äôs first try K = 3. In this case, we have to find the three closest data points (aka three nearest neighbors) to the new (red) data point. As can be seen from the left side, two of three closest neighbors belong to Class B (green) and one belongs to Class A (blue). So, we should assign the new point to Class B.\n\nNow let‚Äôs set K = 5 (right side of above image). In this case, three out of the closest five points belong to Class A, so the new point should be classified as Class A. Unfortunately, there is no specific way of determining K, so we have to try a few values. Very low values of K like 1 or 2 may make the model very complex and sensitive to outliers. A common value for K is 5 [1]."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#pros-and-cons",
    "href": "blog/knn-and-kmeans/index.html#pros-and-cons",
    "title": "What K is in KNN and K-Means",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nFollowing are the advantages and drawbacks of KNN [3]:\nPros\n\nUseful for nonlinear data because KNN is a¬†nonparametric¬†algorithm.\nCan be used for both¬†classification¬†and¬†regression¬†problems, even though mostly used for classification.\n\nCons\n\nDifficult to choose K since there is no statistical way to determine that.\nSlow prediction for large datasets.\nComputationally expensive since it has to store all the training data (Lazy Learner).\nSensitive to non-normalized dataset.\nSensitive to presence of irrelevant features."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#how-to-find-the-best-k",
    "href": "blog/knn-and-kmeans/index.html#how-to-find-the-best-k",
    "title": "What K is in KNN and K-Means",
    "section": "How to find the best K?",
    "text": "How to find the best K?\nThere are several ways to determine K in the K-Means clustering algorithm:\n\nElbow Method: A common way to determine the number of ideal cluster (K) in K-means. In this approach, we run the K-means with several candidates and calculate the WCSS. The best K is selected based on a trade-off between the model complexity (overfitting) and the WCSS.\nSilhouette Score: A score between -1 and 1 measuring the similarity among points of a cluster and comparing that with other clusters. A score of -1 indicates that a point is in the wrong cluster, whereas a score of 1 indicates that the point is in the right cluster [4].\ngap statistics: A method to estimate the number of clusters in a dataset. gap statistic compares the change in the within-cluster variation of output of any clustering technique with an expected reference null distribution [5].\n\nWe usually normalize/standardize continuous variables in the data preprocessing stage in order to avoid variables with much larger values dominating any modeling or analysis process 6."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#pros-and-cons-1",
    "href": "blog/knn-and-kmeans/index.html#pros-and-cons-1",
    "title": "What K is in KNN and K-Means",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nSome pros and cons of K-Means are given below.\nPros\n\nHigh scalability since most of calculations can be run in parallel.\n\nCons\n\nThe outliers can skew the centroids of clusters.\nPoor performance in higher dimensional."
  }
]