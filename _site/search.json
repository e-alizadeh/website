[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{essi2022,\n  author = {Essi},\n  editor = {},\n  title = {Post {With} {Code}},\n  date = {2022-09-14},\n  url = {https://ealizadeh.com/blog/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. 2022. ‚ÄúPost With Code.‚Äù September 14, 2022. https://ealizadeh.com/blog/post-with-code."
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{o'malley2022,\n  author = {Tristan O‚ÄôMalley},\n  editor = {},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2022-09-11},\n  url = {https://ealizadeh.com/blog/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTristan O‚ÄôMalley. 2022. ‚ÄúWelcome To My Blog.‚Äù September 11,\n2022. https://ealizadeh.com/blog/welcome."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9 min\n\n\n\n\n\n\nJan 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\nJun 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 min\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 min\n\n\n\n\n\n\nDec 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\nNov 6, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "I‚Äôm an engineer and a data scientist.\n~ In Permanent Beta: Learning, Improving, Evolving ~"
  },
  {
    "objectID": "blog/automate-github-actions-cron/index.html",
    "href": "blog/automate-github-actions-cron/index.html",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{essi2022,\n  author = {Essi},\n  editor = {},\n  title = {Automate {Your} {Workflow} with {GitHub} {Actions} and\n    {Cron}},\n  date = {2022-01-20},\n  url = {https://ealizadeh.com/blog/automate-github-actions-cron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. 2022. ‚ÄúAutomate Your Workflow with GitHub Actions and\nCron.‚Äù January 20, 2022. https://ealizadeh.com/blog/automate-github-actions-cron."
  },
  {
    "objectID": "blog/automate-github-actions-cron/index.html#header-2",
    "href": "blog/automate-github-actions-cron/index.html#header-2",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Header 2",
    "text": "Header 2\nThis is a post with executable code.\n\nHeader 3\nThis is a po"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html",
    "href": "blog/automate-workflow-github-cron/index.html",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†Towards Data Science blog."
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#considerations-before-automation",
    "href": "blog/automate-workflow-github-cron/index.html#considerations-before-automation",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Considerations before automation",
    "text": "Considerations before automation\nYour script will most likely be different. However, here the following tips may help you get started:\nFirst, run the script on your system. Once you‚Äôre happy with the result and you want to automate the workflow, then use the instructions below to setup a scheduled workflow using GitHub Actions.\nWhen developing an automation task, it is always good to think about how to run it starting from a fresh OS! It is as if you have a new system and you try to run your script there. A few question to ask yourself:\n\nWhere should I start?\nWhat are software/libraries I need to install before running the script?\nWhere can I find the script I want to run?\nDo I need to pass some environment variables or sensitive information like passwords?\n\nHow should I pass sensitive information like passwords or tokens?\n\n\nI will answer above questions for my workflow. Hopefully this will give you enough information to automate your task!"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#cron-job-examples",
    "href": "blog/automate-workflow-github-cron/index.html#cron-job-examples",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Cron job examples",
    "text": "Cron job examples\nBelow examples covers different aspects of a cron syntax and all valid characters (* , - /).\n\nYou can specify your schedule by choosing a valid number for each part. * means ‚Äúevery‚Äù (* * * * * means at every minute on every hour of every day of the month in every month at every day of the week üôÇ). Another example is 30 13 1 * * meaning at 13:30 on day 1 of the month.\nYou can have multiple parameters for a given section by using the value list separator ,. For instance, * * * * 0,3 means every minute only on Sunday and Wednesday.\nYou can have step values by using /. For instance, /10 * * * * means every 10 minutes.\nYou can have a range of values by using dash -. For instance, 4-5 1-10 1 * means every minute between 04:00 - 05:59 AM between day 1 and day 10 of January.\n\nAnd of course, you can have a combination of above options. For example, */30 1-5 * 1,6 0,1 means every 30 minutes between 01:00-05:59 AM only on Sunday and Monday in January and June.\nCheck¬†crontab or crontab guru¬†to come up with the cron syntax for your schedule."
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#where-should-we-start-from",
    "href": "blog/automate-workflow-github-cron/index.html#where-should-we-start-from",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Where should we start from?",
    "text": "Where should we start from?\nWe can start from a fresh Ubuntu system. So, we have the section below the jobs specifying runs-on: ubuntu-latest that will configures the job to run on a fresh virtual machine containing the latest version of an Ubuntu Linux.\nNext step is to clone the current repo. You can achieve this by using uses keyword allowing us to use any action from the GitHub Actions Marketplace . We can use the master branch of actions/checkout here (you can also specify the version like actions/checkout@v2).\n- name: üçΩÔ∏è Checkout the repo\n  uses: actions/checkout@master\n  with:\n    fetch-depth: 1"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#which-softwarelibraries-we-must-install",
    "href": "blog/automate-workflow-github-cron/index.html#which-softwarelibraries-we-must-install",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Which software/libraries we must install?",
    "text": "Which software/libraries we must install?\nThis step is only necessary if you have to install a library. In my case, I have to first install Python 3.8. This can be achieved by using the actions/setup-python@v2 GitHub Action. Afterwards, we want to install the python package. We can install the Zotero2Readwise¬†package by running pip install zotero2readwise. However, in order to execute a command on the runner, we have to use the run keyword.\n- name: üêç Set up Python 3.8\n  uses: actions/setup-python@v2\n  with:\n    python-version: '3.8'\n\n- name: üíø Install Zotero2Readwise Python package\n  run: pip install zotero2readwise"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#where-can-i-find-the-script-i-want-to-run",
    "href": "blog/automate-workflow-github-cron/index.html#where-can-i-find-the-script-i-want-to-run",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Where can I find the script I want to run?",
    "text": "Where can I find the script I want to run?\nIf the script you are trying to run lives in the same repository, you can just skip this step. But here, since the Python script I want to run lives in another GitHub repository, I have to download the script using the curl Linux command.\n- name: üì• Download the Python script needed for automation\n  run:  curl https://raw.githubusercontent.com/e-alizadeh/Zotero2Readwise/master/zotero2readwise/run.py -o run.py"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#run-the-script",
    "href": "blog/automate-workflow-github-cron/index.html#run-the-script",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Run the script",
    "text": "Run the script\nNow that we have set up our environment, we can run the script as mentioned earlier in the Requirements section.\nBut one last point is that since we need to pass some sensitive information (like tokens), we can achieve that by passing the secrets to Settings ‚Üí Secrets ‚Üí New repository secret.\n\n\n\n\n\nHow to pass secrets to the environment of a GitHub repository\n\n\nFigure¬†1: ?(caption)\n\n\nThese secrets will then be available using the following syntax: ${{ secrets.YOUR_SECRET_NAME }} in your YAML file.\nFor more information about handling variables and secrets, you can check the following two pages on the GitHub Docs about Environment variables and Encrypted secrets.\nNow that we have added our secrets, we can run the script as following:\n- name: üöÄ Run Automation\n  run: python run.py ${{ secrets.READWISE_TOKEN }} ${{ secrets.ZOTERO_KEY }} ${{ secrets.ZOTERO_ID }}"
  },
  {
    "objectID": "blog/automate-workflow-github-cron/index.html#putting-everything-together",
    "href": "blog/automate-workflow-github-cron/index.html#putting-everything-together",
    "title": "Automate Your Workflow with GitHub Actions and Cron",
    "section": "Putting everything together",
    "text": "Putting everything together\nThe file containing all steps above is shown below. The file lives on GitHub.\nname: Zotero to Readwise Automation\n\non:\n  push:\n    branches:\n      - master\n  schedule:\n    - cron: \"0 3 * * 1,3,5\" # Runs at 03:00 AM (UTC) every Monday, Wednesday, and Friday (Check https://crontab.guru/)\n\njobs:\n  zotero-to-readwise-automation:\n    runs-on: ubuntu-latest\n    steps:\n      - name: üçΩÔ∏è Checkout the repo\n        uses: actions/checkout@master\n        with:\n          fetch-depth: 1\n\n      - name: üêç Set up Python 3.8\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n\n      - name: üíø Install Zotero2Readwise Python package\n        run: pip install zotero2readwise\n\n      - name: üì• Download the Python script needed for automation\n        run:  curl https://raw.githubusercontent.com/e-alizadeh/Zotero2Readwise/master/zotero2readwise/run.py -o run.py\n\n      - name: üöÄ Run Automation\n        run: python run.py ${{ secrets.READWISE_TOKEN }} ${{ secrets.ZOTERO_KEY }} ${{ secrets.ZOTERO_ID }}\n\n\n\nA screenshot of the GitHub Actions showing how the workflow is run on a schedule or via a push to the master branch."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html",
    "href": "blog/pandas-tutor-tool/index.html",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†Towards Data Science blog."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#pandastutor-creators",
    "href": "blog/pandas-tutor-tool/index.html#pandastutor-creators",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "PandasTutor Creators",
    "text": "PandasTutor Creators\nPandas Tutor was created by Sam Lau¬†and¬†Philip Guo at UC San Diego. This tool is mainly developed for teaching purposes as its creator stated here. This explains some of the limitations this tool have (I will cover some of those limitations later in the post).\nA similar tool called Tidy Data Tutor but for R users is created by Sean Kross¬†and¬†Philip Guo."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#dataset",
    "href": "blog/pandas-tutor-tool/index.html#dataset",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Dataset",
    "text": "Dataset\nLet‚Äôs use the Heart Failure Prediction Dataset Kaggle Dataset (available here). The data is available under¬†Open Database (ODbl) License¬†allowing¬†‚Äúusers to freely share, modify, and use this Database while maintaining this same freedom for others.‚Äù Since Pandas Tutor only works with small data, I will take the first 50 rows of hearts data)."
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#code",
    "href": "blog/pandas-tutor-tool/index.html#code",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Code",
    "text": "Code\nBelow is the code used for the visualization in this post. You may notice that the CSV data is encoded here which is a current limitation of this tool.\nimport pandas as pd\nimport io\n\ncsv = '''\nAge,Sex,ChestPainType,RestingBP,Cholesterol,FastingBS,RestingECG,MaxHR,ExerciseAngina,Oldpeak,ST_Slope,HeartDisease\n40,M,ATA,140,289,0,Normal,172,N,0,Up,0\n49,F,NAP,160,180,0,Normal,156,N,1,Flat,1\n37,M,ATA,130,283,0,ST,98,N,0,Up,0\n48,F,ASY,138,214,0,Normal,108,Y,1.5,Flat,1\n54,M,NAP,150,195,0,Normal,122,N,0,Up,0\n39,M,NAP,120,339,0,Normal,170,N,0,Up,0\n45,F,ATA,130,237,0,Normal,170,N,0,Up,0\n54,M,ATA,110,208,0,Normal,142,N,0,Up,0\n37,M,ASY,140,207,0,Normal,130,Y,1.5,Flat,1\n48,F,ATA,120,284,0,Normal,120,N,0,Up,0\n37,F,NAP,130,211,0,Normal,142,N,0,Up,0\n58,M,ATA,136,164,0,ST,99,Y,2,Flat,1\n39,M,ATA,120,204,0,Normal,145,N,0,Up,0\n49,M,ASY,140,234,0,Normal,140,Y,1,Flat,1\n42,F,NAP,115,211,0,ST,137,N,0,Up,0\n54,F,ATA,120,273,0,Normal,150,N,1.5,Flat,0\n38,M,ASY,110,196,0,Normal,166,N,0,Flat,1\n43,F,ATA,120,201,0,Normal,165,N,0,Up,0\n60,M,ASY,100,248,0,Normal,125,N,1,Flat,1\n36,M,ATA,120,267,0,Normal,160,N,3,Flat,1\n43,F,TA,100,223,0,Normal,142,N,0,Up,0\n44,M,ATA,120,184,0,Normal,142,N,1,Flat,0\n49,F,ATA,124,201,0,Normal,164,N,0,Up,0\n44,M,ATA,150,288,0,Normal,150,Y,3,Flat,1\n40,M,NAP,130,215,0,Normal,138,N,0,Up,0\n36,M,NAP,130,209,0,Normal,178,N,0,Up,0\n53,M,ASY,124,260,0,ST,112,Y,3,Flat,0\n52,M,ATA,120,284,0,Normal,118,N,0,Up,0\n53,F,ATA,113,468,0,Normal,127,N,0,Up,0\n51,M,ATA,125,188,0,Normal,145,N,0,Up,0\n53,M,NAP,145,518,0,Normal,130,N,0,Flat,1\n56,M,NAP,130,167,0,Normal,114,N,0,Up,0\n54,M,ASY,125,224,0,Normal,122,N,2,Flat,1\n41,M,ASY,130,172,0,ST,130,N,2,Flat,1\n43,F,ATA,150,186,0,Normal,154,N,0,Up,0\n32,M,ATA,125,254,0,Normal,155,N,0,Up,0\n65,M,ASY,140,306,1,Normal,87,Y,1.5,Flat,1\n41,F,ATA,110,250,0,ST,142,N,0,Up,0\n48,F,ATA,120,177,1,ST,148,N,0,Up,0\n48,F,ASY,150,227,0,Normal,130,Y,1,Flat,0\n54,F,ATA,150,230,0,Normal,130,N,0,Up,0\n54,F,NAP,130,294,0,ST,100,Y,0,Flat,1\n35,M,ATA,150,264,0,Normal,168,N,0,Up,0\n52,M,NAP,140,259,0,ST,170,N,0,Up,0\n43,M,ASY,120,175,0,Normal,120,Y,1,Flat,1\n59,M,NAP,130,318,0,Normal,120,Y,1,Flat,0\n37,M,ASY,120,223,0,Normal,168,N,0,Up,0\n50,M,ATA,140,216,0,Normal,170,N,0,Up,0\n36,M,NAP,112,340,0,Normal,184,N,1,Flat,0\n41,M,ASY,110,289,0,Normal,170,N,0,Flat,1\n'''\n\ndf_hearts = pd.read_csv(io.StringIO(csv))\ndf_hearts = df_hearts[\n    [\"Age\", \"Sex\", \"RestingBP\", \"ChestPainType\", \"Cholesterol\", \"HeartDisease\"]\n]\n\n(df_hearts.sort_values(\"Age\")\n.groupby([\"Sex\", \"HeartDisease\"])\n.agg({\"RestingBP\": [\"mean\", \"std\"], \n      \"Cholesterol\": [\"mean\", \"std\"],\n      \"Sex\": [\"count\"]\n      })\n)\nSo our transformations is only the last few lines\n(df_hearts.sort_values(\"Age\")\n.groupby([\"Sex\", \"HeartDisease\"])\n.agg({\"RestingBP\": [\"mean\", \"std\"], \n      \"Cholesterol\": [\"mean\", \"std\"],\n      \"Sex\": [\"count\"]\n      })\n)"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#results",
    "href": "blog/pandas-tutor-tool/index.html#results",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Results",
    "text": "Results\n\nStep 1: Sorting the DataFrame\n\n\n\nVisualization of the sort_values() result (steps 1) (generated using PandasTutor)\n\n\nVisualization of the sort_values() result (steps 1) (generated using PandasTutor)\n\n\nStep 2: Visualize Pandas Groupby operation\nAfter sorting the results in Step 1 and visualizing it, we can visualize the groupby() operation\n\n\n\nVisualization of the groupby() result (steps 1 and 2) (generated using PandasTutor)\n\n\n\n\nStep 3: Calculate different aggregations on multiple columns\nHere, I will be calculating the mean and standard deviation of two columns ‚ÄúRestingBP‚Äù and ‚ÄúCholesterol‚Äù and also provide a count for each group (here I‚Äôm using the ‚ÄúSex‚Äù column to get that information.)\n\n\n\nVisualization of the final result that is the aggregation (steps 1 - 3) (generated using PandasTutor)\n\n\nVisualization of the final result that is the aggregation (steps 1 - 3) (generated using PandasTutor)\n\n\nInteresting sharing feature\nPandas Tutor also provides you with a shareable URL that even includes the CSV data used in the transformation. For instance, you can check my transformation code and results here or via below link!\nhttps://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20io%0A%0Acsv%20%3D%20'''%0AAge,Sex,ChestPainType,RestingBP,Cholesterol,FastingBS,RestingECG,MaxHR,ExerciseAngina,Oldpeak,ST_Slope,HeartDisease%0A40,M,ATA,140,289,0,Normal,172,N,0,Up,0%0A49,F,NAP,160,180,0,Normal,156,N,1,Flat,1%0A37,M,ATA,130,283,0,ST,98,N,0,Up,0%0A48,F,ASY,138,214,0,Normal,108,Y,1.5,Flat,1%0A54,M,NAP,150,195,0,Normal,122,N,0,Up,0%0A39,M,NAP,120,339,0,Normal,170,N,0,Up,0%0A45,F,ATA,130,237,0,Normal,170,N,0,Up,0%0A54,M,ATA,110,208,0,Normal,142,N,0,Up,0%0A37,M,ASY,140,207,0,Normal,130,Y,1.5,Flat,1%0A48,F,ATA,120,284,0,Normal,120,N,0,Up,0%0A37,F,NAP,130,211,0,Normal,142,N,0,Up,0%0A58,M,ATA,136,164,0,ST,99,Y,2,Flat,1%0A39,M,ATA,120,204,0,Normal,145,N,0,Up,0%0A49,M,ASY,140,234,0,Normal,140,Y,1,Flat,1%0A42,F,NAP,115,211,0,ST,137,N,0,Up,0%0A54,F,ATA,120,273,0,Normal,150,N,1.5,Flat,0%0A38,M,ASY,110,196,0,Normal,166,N,0,Flat,1%0A43,F,ATA,120,201,0,Normal,165,N,0,Up,0%0A60,M,ASY,100,248,0,Normal,125,N,1,Flat,1%0A36,M,ATA,120,267,0,Normal,160,N,3,Flat,1%0A43,F,TA,100,223,0,Normal,142,N,0,Up,0%0A44,M,ATA,120,184,0,Normal,142,N,1,Flat,0%0A49,F,ATA,124,201,0,Normal,164,N,0,Up,0%0A44,M,ATA,150,288,0,Normal,150,Y,3,Flat,1%0A40,M,NAP,130,215,0,Normal,138,N,0,Up,0%0A36,M,NAP,130,209,0,Normal,178,N,0,Up,0%0A53,M,ASY,124,260,0,ST,112,Y,3,Flat,0%0A52,M,ATA,120,284,0,Normal,118,N,0,Up,0%0A53,F,ATA,113,468,0,Normal,127,N,0,Up,0%0A51,M,ATA,125,188,0,Normal,145,N,0,Up,0%0A53,M,NAP,145,518,0,Normal,130,N,0,Flat,1%0A56,M,NAP,130,167,0,Normal,114,N,0,Up,0%0A54,M,ASY,125,224,0,Normal,122,N,2,Flat,1%0A41,M,ASY,130,172,0,ST,130,N,2,Flat,1%0A43,F,ATA,150,186,0,Normal,154,N,0,Up,0%0A32,M,ATA,125,254,0,Normal,155,N,0,Up,0%0A65,M,ASY,140,306,1,Normal,87,Y,1.5,Flat,1%0A41,F,ATA,110,250,0,ST,142,N,0,Up,0%0A48,F,ATA,120,177,1,ST,148,N,0,Up,0%0A48,F,ASY,150,227,0,Normal,130,Y,1,Flat,0%0A54,F,ATA,150,230,0,Normal,130,N,0,Up,0%0A54,F,NAP,130,294,0,ST,100,Y,0,Flat,1%0A35,M,ATA,150,264,0,Normal,168,N,0,Up,0%0A52,M,NAP,140,259,0,ST,170,N,0,Up,0%0A43,M,ASY,120,175,0,Normal,120,Y,1,Flat,1%0A59,M,NAP,130,318,0,Normal,120,Y,1,Flat,0%0A37,M,ASY,120,223,0,Normal,168,N,0,Up,0%0A50,M,ATA,140,216,0,Normal,170,N,0,Up,0%0A36,M,NAP,112,340,0,Normal,184,N,1,Flat,0%0A41,M,ASY,110,289,0,Normal,170,N,0,Flat,1%0A'''%0A%0Adf_hearts%20%3D%20pd.read_csv%28io.StringIO%28csv%29%29%0Adf_hearts%20%3D%20df_hearts%5B%0A%20%20%20%20%5B%22Age%22,%20%22Sex%22,%20%22RestingBP%22,%20%22ChestPainType%22,%20%22Cholesterol%22,%20%22HeartDisease%22%5D%0A%5D%0A%0A%28df_hearts.sort_values%28%22Age%22%29%0A.groupby%28%5B%22Sex%22,%20%22HeartDisease%22%5D%29%0A.agg%28%7B%22RestingBP%22%3A%20%5B%22mean%22,%20%22std%22%5D,%20%0A%20%20%20%20%20%20%22Cholesterol%22%3A%20%5B%22mean%22,%20%22std%22%5D,%0A%20%20%20%20%20%20%22Sex%22%3A%20%5B%22count%22%5D%0A%20%20%20%20%20%20%7D%29%0A%29&d=2021-12-08&lang=py&v=v1"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#pros",
    "href": "blog/pandas-tutor-tool/index.html#pros",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Pros:",
    "text": "Pros:\n\nStep-by-step visualization\nInteractive plots (you can track the data rows before and after the transformation)\nShareable URL"
  },
  {
    "objectID": "blog/pandas-tutor-tool/index.html#cons-current-limitations",
    "href": "blog/pandas-tutor-tool/index.html#cons-current-limitations",
    "title": "Visualize your Pandas Data Transformation using PandasTutor",
    "section": "Cons (current limitations):",
    "text": "Cons (current limitations):\n\nOnly works for small codes (The code should be 5000bytes). Since the data is also encoded and not read from a file, hence, you can only visualize small datasets.\nAs stated in the previous step, you have to encode the data along with the code as reading from external resources (files or links) are not supported.\nLimited Pandas‚Äô methods support.\nYou can visualize the Pandas expression only on the last line. You may have to pipe multiple steps together or run the visualizations separately.\n\nFor a complete list of unsupported features or other FAQ, you can check here."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html",
    "href": "blog/dbt-tutorial/index.html",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†KDnuggets."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#signup",
    "href": "blog/dbt-tutorial/index.html#signup",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Signup",
    "text": "Signup\nYou can sign up at getdbt.com. The free plan is a great plan for small projects and testing."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#database-with-populated-data",
    "href": "blog/dbt-tutorial/index.html#database-with-populated-data",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Database with populated data",
    "text": "Database with populated data\nYou can check my post on how to deploy a free PostgreSQL database on Heroku. The post provides step-by-step instructions on how to do it.\nYou can also check the data ingestion script in the GitHub repo accompanying this article.\ne-alizadeh/sample_dbt_project\nFollowing the above, we have generated two tables in a PostgreSQL database that we are going to use in this post. There are two tables in the database, namely covid_latest and population_prosperity. You can find the ingestion script on the GitHub repo for this post."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-cli-installation",
    "href": "blog/dbt-tutorial/index.html#dbt-cli-installation",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt CLI Installation",
    "text": "dbt CLI Installation\nYou can install the dbt command-line interface (CLI) by following the instructions on the following dbt documentation page.\nInstallation | dbt Docs"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#how-to-use-dbt",
    "href": "blog/dbt-tutorial/index.html#how-to-use-dbt",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "How to use dbt?",
    "text": "How to use dbt?\nA dbt project is a directory containing .sql and .yml files. The minimum required files are:\n\nA project file named dbt_project.yml: This file contains configurations of a dbt project.\nModel(s) (.sql files): A model in dbt is simply a single .sql file containing a single select statement.\n\nEvery dbt project needs a dbt_project.yml file ‚Äî this is how dbt knows a directory is a dbt project. It also contains important information that tells dbt how to operate on your project.\nYou can find more information about dbt projects here.\n\n\n\n\n\n\nNote\n\n\n\nüí° A dbt model is basically a .sql file with a SELECT statement."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-commands",
    "href": "blog/dbt-tutorial/index.html#dbt-commands",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt Commands",
    "text": "dbt Commands\ndbt commands start with dbt and can be executed using one of the following ways:\n\ndbt Cloud (the command section at the bottom of the dbt Cloud dashboard),\ndbt CLI\n\nSome commands can only be used in dbt CLI like dbt init. Some dbt commands we will use in this post are\n\ndbt init (only in dbt CLI)\ndbt run\ndbt test\ndbt docs generate"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-1-initialize-a-dbt-project-sample-files-using-dbt-cli",
    "href": "blog/dbt-tutorial/index.html#step-1-initialize-a-dbt-project-sample-files-using-dbt-cli",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 1: Initialize a dbt project (sample files) using dbt CLI",
    "text": "Step 1: Initialize a dbt project (sample files) using dbt CLI\nYou can use [dbt init](https://docs.getdbt.com/reference/commands/init) to generate sample files/folders. In particular, dbt init project_name will create the following:\n\na¬†~/.dbt/profiles.yml¬†file if one does not already exist\na new folder called¬†[project_name]\ndirectories and sample files necessary to get started with dbt\n\n\n\n\n\n\n\nWarning\n\n\n\nSince dbt init generates a directory namedproject_name, and in order to avoid any conflict, you should not have any existing folder with an identical name.\n\n\n\n\n\ndbt init \n\n\nThe result is a directory with the following sample files.\nsample_dbt_project\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ analysis\n‚îú‚îÄ‚îÄ data\n‚îú‚îÄ‚îÄ dbt_project.yml\n‚îú‚îÄ‚îÄ macros\n‚îú‚îÄ‚îÄ models\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ example\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ my_first_dbt_model.sql\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ my_second_dbt_model.sql\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ schema.yml\n‚îú‚îÄ‚îÄ snapshots\n‚îî‚îÄ‚îÄ tests\nFor this post, we will just consider the minimum files and remove the extra stuff.\nsample_dbt_project\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ dbt_project.yml\n‚îî‚îÄ‚îÄ models\n ¬†¬† ‚îú‚îÄ‚îÄ my_first_dbt_model.sql\n    ‚îú‚îÄ‚îÄ my_second_dbt_model.sql\n¬†   ‚îî‚îÄ‚îÄ schema.yml"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-2-set-up-a-git-repository",
    "href": "blog/dbt-tutorial/index.html#step-2-set-up-a-git-repository",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 2: Set Up a Git Repository",
    "text": "Step 2: Set Up a Git Repository\nYou can use an existing repo, as specified during the setup. You can configure the repositories by following the dbt documentation here.\n\nOr, if you want to create a new repo‚Ä¶\nyou can create a new repository from inside the created directory. You can do that as below\ngit init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origing <repo_url>\ngit push -u origin master"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#step-3-set-up-a-new-project-on-dbt-cloud-dashboard",
    "href": "blog/dbt-tutorial/index.html#step-3-set-up-a-new-project-on-dbt-cloud-dashboard",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Step 3: Set Up a New Project on dbt Cloud Dashboard",
    "text": "Step 3: Set Up a New Project on dbt Cloud Dashboard\nIn the previous step, we created a sample dbt project containing sample models and configurations. Now, we want to create a new project and connect our database and repository on the dbt Cloud dashboard.\nBefore we continue, you should have\n\nsome data already available in a database,\na repository with the files generated at the previous step\n\nYou can follow the steps below to set up a new project in dbt Cloud (keep in mind this step is different than the previous step in that we only generated some sample files).\n\n\nSet up a new dbt project on dbt Cloud\nThe dbt_project.yml file for our project is shown below (you can find the complete version in the GitHub repo to this post).\nname: 'my_new_project'\nversion: '1.0.0'\nconfig-version: 2\n\nvars:\n  selected_country: USA\n    selected_year: 2019\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'default'\n\n# There are other stuff that are generated automatically when you run `dbt init`"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#dbt-models",
    "href": "blog/dbt-tutorial/index.html#dbt-models",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "dbt models",
    "text": "dbt models\nLet‚Äôs create simple dbt models that retrieve few columns of the tables.\nselect \"iso_code\", \"total_cases\", \"new_cases\" from covid_latest\nselect \"code\", \"year\", \"continent\", \"total_population\" from population_prosperity\n\n\n\n\n\n\nWarning\n\n\n\nThe dbt model name is the filename of the sql file in the models directory. The model name may differ from the table name in the database. For instance, in above, the dbt model population is the result of a SELECT statement on population_prosperity table in the database.\n\n\n\nRun models\nYou can run all models in your dbt project by executing dbt run. A sample dbt run output is shown below. You can see a summary or detailed log of running all dbt models. This helps a lot to debug any issue you may have in the queries. For instance, you can see a failed model that throws a Postgres error.\n\n\n\nDetailed log of failed jinja_and_variable_usage dbt model"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#jinja-macros",
    "href": "blog/dbt-tutorial/index.html#jinja-macros",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Jinja & Macros",
    "text": "Jinja & Macros\ndbt uses Jinja templating language, making a dbt project an ideal programming environment for SQL. With Jinja, you can do transformations that are not normally possible in SQL, like using environment variables, or macros ‚Äî abstract snippets of SQL, which is analogous to functions in most programming languages. Whenever you see a {{ ... }}, you‚Äôre already using Jinja. For more information about Jinja and additional Jinja-style functions defined, you can check dbt documentation.\nLater in this post, we will cover custom macros defined by dbt."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#using-variables",
    "href": "blog/dbt-tutorial/index.html#using-variables",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Using Variables",
    "text": "Using Variables\n\nDefine a variable\nYou can define your variables under the vars section in your dbt_project.yml. For instance, let‚Äôs define a variable called selected_country whose default value is USA and another one called selected_year whose default value is 2019.\nname: 'my_new_project'\nversion: '1.0.0'\nconfig-version: 2\n\nvars:\n  selected_country: USA\n    selected_year: 2019\n\n\nUse a Variable\nYou can use variables in your dbt models via [var()](https://docs.getdbt.com/reference/dbt-jinja-functions/var) Jinja function ({{ var(\"var_key_name\") }} ."
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#macros",
    "href": "blog/dbt-tutorial/index.html#macros",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Macros",
    "text": "Macros\nThere are many useful transformations and useful macros in dbt_utils that can be used in your project. For a list of all available macros, you can check their GitHub repo.\nNow, let‚Äôs add dbt_utils to our project and install it by following the below steps:\n\nAdd dbt_utils macro to your¬†packages.yml¬†file, as follows:\n\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 0.6.6\n\nRun¬†dbt deps¬†to install the package.\n\n\n\n\nInstall packages using dbt deps"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#complex-dbt-models",
    "href": "blog/dbt-tutorial/index.html#complex-dbt-models",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Complex dbt models",
    "text": "Complex dbt models\nThe models (selects) are usually stacked on top of one another. For building more complex models, you will have to use [ref()](https://docs.getdbt.com/reference/dbt-jinja-functions/ref) macro. ref() is the most important function in dbt as it allows you to refer to other models. For instance, you may have a model (aka SELECT query) that does multiple stuff, and you don‚Äôt want to use it in other models. It will be difficult to build a complex model without using macros introduced earlier.\n\ndbt model using ref() and global variables\nWe can build more complex models using the two dbt models defined earlier in the post. For instance, let‚Äôs create a new dbt model that joins the above two tables on the country code and then filters based on selected country and year.\nselect *\nfrom {{ref('population')}} \ninner join {{ref('covid19_latest_stats')}} \non {{ref('population')}}.code = {{ref('covid19_latest_stats')}}.iso_code \nwhere code='{{ var(\"selected_country\") }}' AND year='{{ var(\"selected_year\") }}'\nFew points about the query above:\n\n{{ref('dbt_model_name')}}is used to refer to dbt models available in the project.\nYou can get a column from the model like {{ref('dbt_model_name')}}.column_name.\nYou can use variables defined in dbt_project.yml file by {{var(\"variable_name)}}.\n\nThe abbove code snippet joins the data from population and covid19_latest_stats models on the country code and filters them based on the selected_country=USA and selected_year=2019. The output of the model is shown below.\n\n\n\nThe output of the jinja_and_variable_usage dbt model\n\n\nYou can also see the compiled SQL code snippet by clicking on compile sql button. This is very useful particularly if you want to run the query outside the dbt tool.\n\n\n\nCompiled SQL code for jinja_and_variable_usage dbt model\n\n\n\n\ndbt model using dbt_utils package and macros\ndbt_utils package contains macros (aka functions) you can use in your dbt projects. A list of all macros is available on dbt_utils‚Äô GitHub page.\nLet‚Äôs use dbt_utils [pivot()](https://github.com/dbt-labs/dbt-utils/#pivot-source) and [get_column_values()](https://github.com/dbt-labs/dbt-utils/#get_column_values-source) macros in a dbt model as below:\nselect\n  continent,\n  {{ dbt_utils.pivot(\n      \"population.year\",\n      dbt_utils.get_column_values(ref('population'), \"year\")\n  ) }}\nfrom {{ ref('population') }}\ngroup by continent\nThe above dbt model will compile to the following SQL query in dbt.\nselect\n  continent,\n    sum(case when population.year = '2015' then 1 else 0 end) as \"2015\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2017\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2016\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2018\",\n        sum(case when population.year = '2017' then 1 else 0 end) as \"2019\"\nfrom \"d15em1n30ihttu\".\"dbt_ealizadeh\".\"population\"\ngroup by continent\nlimit 500\n/* limit added automatically by dbt cloud */"
  },
  {
    "objectID": "blog/dbt-tutorial/index.html#database-administration-using-hooks-operations",
    "href": "blog/dbt-tutorial/index.html#database-administration-using-hooks-operations",
    "title": "dbt for Data Transformation - A Hands-on Tutorial",
    "section": "Database administration using Hooks & Operations",
    "text": "Database administration using Hooks & Operations\nThere are database management tasks that require running additional SQL queries, such as:\n\nCreate user-defined functions\nGrant privileges on a table\nand many more\n\ndbt has two interfaces (hooks and operations) for executing these tasks and importantly version control them. Hooks and operations are briefly introduced here. For more info, you can check dbt documentation.\n\nHooks\nHooks are simply SQL snippets that are executed at different times. Hooks are defined in the dbt_project.yml file. Different hooks are:\n\npre-hook: executed before a model is built\npost-hook: executed after a model is built\non-run-start: executed at the start of dbt run\non-run-end: executed at the end of dbt run\n\n\n\nOperations\nOperations are a convenient way to invoke a macro without running a model. Operations are triggered using [dbt run-operation](https://docs.getdbt.com/reference/commands/run-operation) command.\nNote that, unlike hooks, you need to explicitly execute the SQL in a dbt operation."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html",
    "href": "blog/knn-and-kmeans/index.html",
    "title": "What K is in KNN and K-Means",
    "section": "",
    "text": "In this post, we will go over two popular machine learning algorithms: K-Nearest Neighbors (aka KNN) and K-Means, and what K stands for in each algorithm. An overview of both popular ML techniques (including a visual illustration) will be provided.\nBy the end of this post, we will be able to answer the following questions:\n\nWhat‚Äôs the difference between KNN and K-Means?\nWhat does K mean in KNN and K-Means?\nWhat is a nonparametric model?\nWhat is a lazy learner model?\nWhat is within-cluster sum of squares, WCSS (aka intracluster inertia/distance, within-cluster variance)?\nHow to determine the best value K in K-Means?\nWhat are pros and cons of KNN?\nWhat are pros and cons of K-Means?\n\n\n\n\n\n\n\nNote\n\n\n\nüëâ The goal of this post is not to compare KNN and K-Means as each one addresses a different problem. Hence, comparing them is like comparing apples to oranges."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#an-illustration-of-k-nn",
    "href": "blog/knn-and-kmeans/index.html#an-illustration-of-k-nn",
    "title": "What K is in KNN and K-Means",
    "section": "An Illustration of K-NN",
    "text": "An Illustration of K-NN\nAs I mentioned earlier, KNN is a supervised learning technique, so we should have a labeled dataset. Let‚Äôs say we have two classes as can be seen in below image: Class A (blue points) and Class B (green points). A new data point (red) is given to us and we want to predict whether the new point belongs to Class A or Class B.\nLet‚Äôs first try K = 3. In this case, we have to find the three closest data points (aka three nearest neighbors) to the new (red) data point. As can be seen from the left side, two of three closest neighbors belong to Class B (green) and one belongs to Class A (blue). So, we should assign the new point to Class B.\n\nNow let‚Äôs set K = 5 (right side of above image). In this case, three out of the closest five points belong to Class A, so the new point should be classified as Class A. Unfortunately, there is no specific way of determining K, so we have to try a few values. Very low values of K like 1 or 2 may make the model very complex and sensitive to outliers. A common value for K is 5 [1]."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#pros-and-cons",
    "href": "blog/knn-and-kmeans/index.html#pros-and-cons",
    "title": "What K is in KNN and K-Means",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nFollowing are the advantages and drawbacks of KNN [3]:\nPros\n\nUseful for nonlinear data because KNN is a¬†nonparametric¬†algorithm.\nCan be used for both¬†classification¬†and¬†regression¬†problems, even though mostly used for classification.\n\nCons\n\nDifficult to choose K since there is no statistical way to determine that.\nSlow prediction for large datasets.\nComputationally expensive since it has to store all the training data (Lazy Learner).\nSensitive to non-normalized dataset.\nSensitive to presence of irrelevant features."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#how-to-find-the-best-k",
    "href": "blog/knn-and-kmeans/index.html#how-to-find-the-best-k",
    "title": "What K is in KNN and K-Means",
    "section": "How to find the best K?",
    "text": "How to find the best K?\nThere are several ways to determine K in the K-Means clustering algorithm:\n\nElbow Method: A common way to determine the number of ideal cluster (K) in K-means. In this approach, we run the K-means with several candidates and calculate the WCSS. The best K is selected based on a trade-off between the model complexity (overfitting) and the WCSS.\nSilhouette Score: A score between -1 and 1 measuring the similarity among points of a cluster and comparing that with other clusters. A score of -1 indicates that a point is in the wrong cluster, whereas a score of 1 indicates that the point is in the right cluster [4].\ngap statistics: A method to estimate the number of clusters in a dataset. gap statistic compares the change in the within-cluster variation of output of any clustering technique with an expected reference null distribution [5].\n\nWe usually normalize/standardize continuous variables in the data preprocessing stage in order to avoid variables with much larger values dominating any modeling or analysis process 6."
  },
  {
    "objectID": "blog/knn-and-kmeans/index.html#pros-and-cons-1",
    "href": "blog/knn-and-kmeans/index.html#pros-and-cons-1",
    "title": "What K is in KNN and K-Means",
    "section": "Pros and Cons",
    "text": "Pros and Cons\nSome pros and cons of K-Means are given below.\nPros\n\nHigh scalability since most of calculations can be run in parallel.\n\nCons\n\nThe outliers can skew the centroids of clusters.\nPoor performance in higher dimensional."
  },
  {
    "objectID": "blog/3-ways-to-add-images-to-your-jupyter-notebook/index.html",
    "href": "blog/3-ways-to-add-images-to-your-jupyter-notebook/index.html",
    "title": "3 Ways to Add Images to Your Jupyter Notebook",
    "section": "",
    "text": "Note\n\n\n\nüëâ This article is also published on¬†Better Programming blog.\n\n\n\nIntroduction\nThe Jupyter Notebook (formerly IPython Notebooks) is a popular web-based interactive environment that was first started from the IPython project and is currently maintained by the nonprofit organization¬†Project Jupyter. It‚Äôs a convenient tool to create and share documents that contain codes, equations, texts, and visualizations. A Jupyter Notebook can be easily converted to HTML, LaTeX, PDF, Markdown, Python, and other open standard formats1.\nIn this post, I will present three ways to add images to your notebook. The first two approaches are pretty standard that rely on external resources to illustrate the images, and those are to use the image URL or to load an image from a local file. However, both of these methods rely on external resources. To contain all images used in the notebook within itself without relying on any external source, we can use the Base64 encoding algorithm to encode our images and use those encoded data to illustrate them. So, we will briefly talk about the Base64 algorithm too.\nHere, I will be using the¬†Image¬†class from IPython‚Äôs¬†display¬†module to show all images.\n\n\nApproach 1: Add an image from a local file\nWe can add images from your local drive by providing the path to the file.\nfrom IPython import display\ndisplay.Image(\"./image.png\")\nThere are two downsides to this approach:\n\nThe local or absolute path provided may not work well on another system.\nYou have to make sure to include all images used in a notebook with anyone you want to share. You may end up compressing all files to a single zip file for convenience when sharing your notebook.\n\n\n\nApproach 2: Add an image from a URL\nYou can also add an image to your notebook using the URL link to the image, as shown below.\nfrom IPython import display\ndisplay.Image(\"URL of the image\")\nIn this case, the image provider may remove the image or change the image properties without knowing it. So, let‚Äôs say you have an old notebook that has a broken image link. It might be difficult to retrieve the original image. Even if you are taken the image from your website, you should be careful not to change the image link or properties!\n\n\nApproach 3: Embed an image by Base64 Encode-Decode\nThe first two approaches rely on external resources. In Section¬†2, we used the path to a file that is saved locally. Any change in the filename or path may impact the image in the notebook. In Section¬†3, we rely on a URL, and any change in the original link will impact the image in the notebook. Unlike the previous methods, Approach 3 embeds the image as a text using the¬†Base64 encoding algorithm. This way, we will not be relying on any external resources for the embedded image. Hence, we can have all images embedded in the same notebook file.\nBase64 is a binary-to-text encoding algorithm to convert data (including but not limited to images) as plain text. It is one of the most popular binary-to-text encoding schemes (if not the most one). It‚Äôs widely used in text documents such as HTML, JavaScript, CSS, or XML scripts2. However, technically speaking, you can even encode/decode audio or video files too!!\nFirst, you need to encode your image. For this, you can use the online tool¬†Base64-Image. After you upload your image, you can then click on¬†the copy image, as shown below.\n\n\n\nScreenshot of the uploaded image at base64-image\n\n\nNow you can paste the encoded image code into your notebook, but first, you should remove¬†data:image/png;base64,¬†at the beginning. Don‚Äôt forget to also remove the comma after base64!\nNow that we have the encoded image code, we can use the Python standard¬†base64 library¬†to decode the base64 data, as shown below.\n\nfrom IPython import display\nfrom base64 import b64decode\nbase64_data = \"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\"\n\ndisplay.Image(b64decode(base64_data))\n\n\n\n\nAn image decoded from Base64\n\n\n\n\nAs you may have noticed by now, the main advantage of using Base64 to add all images to your Notebook is the fact that do yo no longer need to worry about any external resources for your images as they are all self-contained in your Notebook. The other point to be aware of is that including the images in your notebook will increase your notebook‚Äôs file size depending on the image resolution.\n\n\nConclusion\nIn this post, we went over three ways to add an image to a Jupyter Notebook, and those are through 1) a URL, 2) a local file, or 3) by Base64 encoding the image data. I also provided a resource link that you can use to Base64 encode your image. The main benefit of using the Base64 encoding scheme is to reduce (or even) remove any external images in your notebook.\n\n\n\n\n\n\nNote\n\n\n\nüìì You can find the notebook for this post on GitHub.\n\n\n\n\n\n\n\nFootnotes\n\n\nWikipedia,¬†Project Jupyter¬†(Accessed on November 16, 2020)‚Ü©Ô∏é\nWikipedia,¬†Base64¬†(Accessed on November 16, 2020)‚Ü©Ô∏é\n\nCitationBibTeX citation:@online{alizadeh2020,\n  author = {Essi Alizadeh},\n  editor = {},\n  title = {3 {Ways} to {Add} {Images} to {Your} {Jupyter} {Notebook}},\n  date = {2020-11-06},\n  url = {https://ealizadeh.com/blog/3-ways-to-add-images-to-your-jupyter-notebook},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi Alizadeh. 2020. ‚Äú3 Ways to Add Images to Your Jupyter\nNotebook.‚Äù November 6, 2020. https://ealizadeh.com/blog/3-ways-to-add-images-to-your-jupyter-notebook."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "",
    "text": "In this article, I will go over 15 cognitive errors from the book ‚ÄúThe Art of Thinking Clearly‚Äù by Rolf Dobelli. Any scientist, analyst, or basically anyone who works with data needs to be familiar with these fallacies. My goal is to present these cognitive errors to think about them in your daily life and avoid falling into one!\nI have created a graph network showing the connections among these cognitive errors. An interactive network graph on all 98 cognitive errors mentioned in the book can be found here. However, since that‚Äôs overwhelming and many of them are not directly related to this article, I have also created the network graph of all biases explained in this article and a few more below."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#base-rate-neglect",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#base-rate-neglect",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "1. Base-Rate Neglect",
    "text": "1. Base-Rate Neglect\nThis fallacy is a common reasoning error where people neglect the distribution of the data in favor of specific individual information. Here is an example of this bias from the book.\nMark is a man from Germany who wears glasses and listens to Mozart. Which one is more likely?\nHe is 1) a truck driver or 2) a literature professor in Frankfurt?\nMost people will bet on Option 2 (the wrong option). The number of truck drivers in Germany is 10,000 times more than the number of literature professors in Frankfurt. Hence, it‚Äôs more likely that Mark is a truck driver[[1]](15%20Cognitive%20Errors%20Every%20Analyst%20Must%20Know%20(+%20Net%20f46d78a24d414957a827ce730e6076a4.md).\n\n1.1 False Positive Paradox\nThe false positive paradox is an example of base-rate bias when the number of false positives is more than the number of true positives[[2]](15%20Cognitive%20Errors%20Every%20Analyst%20Must%20Know%20(+%20Net%20f46d78a24d414957a827ce730e6076a4.md).\nExample: Imagine that 1% of a population is actually infected with a disease, and there is a test with a 5% false-positive rate and no false-negative rate, i.e. False Negative or FN‚ÄÑ=‚ÄÑ0FN=0. The expected outcome of 10000 tests would be\n\nInfected and the test correctly indicates the diseases (True Positive): 10000 \\times \\frac{1}{100} = 100 \\; (TP = 100)\nUninfected and the test incorrectly indicates the person has the disease (False Positive) 10000 \\times \\frac{100 - 1}{100} \\times 0.05 = 495 \\; (FP=495)\n\nSo, a total of 100+495=595 people tested positive. And the remaining 10000‚àí595=9405 \\; (TN=9405) tests have correct negative results (True Negative).\nOverall, only 100 of the 595 positive results are actually correct. The probability of actually being infected when the test results are positive is \\frac{100}{100 + 495} = 0.168 or 16.8\\%, for a test with an accuracy of 95\\% (\\frac{TP + TN}{TP + FP + FN + TN} = \\frac{100 + 9405}{10,000}=0.9505)"
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#clustering-illusion",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#clustering-illusion",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "2. Clustering Illusion",
    "text": "2. Clustering Illusion\n\nThe brain seeks patterns.\n\nThe brain seeks patterns, coherence, and order where none really exists. As per this cognitive bias, we are oversensitive to finding a structure or rule. The human‚Äôs false pattern recognition is also known as apophenia. That is the tendency to make meaningful connections between unrelated things. Examples of such phenomena are gambler‚Äôs fallacy, figures in clouds, and patterns with no deliberate designs. A popular example of this is the ‚ÄúFace on Mars‚Äù as shown below:\n\n\n\nA cropped version of the small part of the Cydonia region, taken by the Viking 1 orbiter and released by NASA/JPL on July 25, 1976. (Source: Wikipedia)\n\n\nThis is prominent in the gambling, misinterpretation of statistics, conspiracy theories, etc. This cognitive error is also linked to two other errors: coincidence and false causality. The way to overcome the clustering illusion, particularly for anyone working with data, is to assume any pattern found in the data as random and statistically test the pattern found (see Beitman 2009)."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#confirmation-bias",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#confirmation-bias",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "3. Confirmation Bias",
    "text": "3. Confirmation Bias\n\n\n\nConfirmation bias\n\n\nThe confirmation bias is the only cognitive error that the book author wrote two parts about it. The following quote from the book says it all:\n\nThe confirmation bias is the mother of all misconceptions. ‚ÄîRolf Dobelli\n\nConfirmation bias is the tendency to interpret new information based on our existing beliefs and discard any opposing evidence (disconfirming evidence). The internet is the main ground for this bias. Think about the YouTube video suggestions made to us based on our watch history. It gives us recommendations to videos that align with our current views/beliefs/theories. We are actually prone to confirmation bias in many platforms that use Recommender systems like Google, Facebook, Twitter, etc."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#overconfidence-effect",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#overconfidence-effect",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "4. Overconfidence Effect",
    "text": "4. Overconfidence Effect\nThis cognitive bias tells us that we systematically overestimate our ability to predict. In other words, this bias says that we believe subjectively that our judgment is better than it objectively is (see Dobelli 2013). No wonder why most major projects are not completed in less time or cheaper than initially predicted. The overconfidence bias is more common among experts. The reason is that experts obviously know more about their own field, but they overestimate exactly how much more."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#regression-to-mean",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#regression-to-mean",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "5. Regression to Mean",
    "text": "5. Regression to Mean\nImagine the weather in your city reaches a record hot weather. The temperature will most likely drop in the next few days, back towards the monthly average. This bias depends on the random variance impacting any measurement, causing some to be extreme. Ignoring this bias leads to overestimating the correlation between the two measures. For instance, if an athlete performs extremely well in a year, we expect a better performance the year after. If that‚Äôs not the case, we may come up with causal relationships instead of considering that we probably overestimated the next year‚Äôs performance!"
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#induction",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#induction",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "6. Induction",
    "text": "6. Induction\nThe inductive bias occurs when we draw universal conclusions from individual observations. For example, all observed people in a city wear glasses. Therefore all the people in that city wear glasses. This can lead to false causality (another bias I will talk about later). For example, every time Bob drinks milk, he gets cramps, and therefore he concludes that he gets cramps because he drinks milk!"
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#intention-to-treat-error",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#intention-to-treat-error",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "7. Intention-To-Treat Error",
    "text": "7. Intention-To-Treat Error\nMainly used in medical research studies, the intention-to-treat (ITT) principle is crucial in interpreting the results of randomized clinical trials. It helps the researchers to assess the true effect of choosing a medical treatment. Let‚Äôs have an example to understand this fallacy better.\nSuppose a pharmaceutical company developed a new drug for heart diseases. A study ‚Äúproves‚Äù that the drug is improving the patient‚Äôs health and reduces the chance of dying from heart diseases. The five-year mortality rate of patients who take the drug regularly is 15%. The company may not tell you that the mortality rate of patients who took the drug irregularly was 30%. So, is the drug a complete success?\nThe point here is that the drug may not be the decisive factor here, but the patient‚Äôs behavior. The patients who didn‚Äôt take the drug according to the schedules may have side effects causing them to stop taking the drug. Hence, these patients will not be in the ‚Äúregular category‚Äù of the study, for which a 15% rate was reported. So, because of the ITT error, the drug looks much more effective than it actually is (see Dobelli 2013)."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#false-causality",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#false-causality",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "8. False Causality",
    "text": "8. False Causality\n\nThe Correlation does not imply causation.\n\nThis fallacy occurs when we wrongly infer a cause-and-effect relationship between two measurements solely based on their correlation. For instance, the per capita consumption of mozzarella cheese is highly correlated with civil engineering PhDs awarded (r=0.9586). Does this mean that the consumption of mozzarella cheese leads to more civil engineering doctorates awarded?\n\n\n\nDoes the consumption of mozzarella cheese actually lead to more civil engineering PhDs awarded? (Graph: TylerVigen.com, Data sources: U.S. Department of Agriculture and National Science Foundation)"
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#the-problem-with-averages",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#the-problem-with-averages",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "9. The Problem with Averages",
    "text": "9. The Problem with Averages\n\nDon‚Äôt cross a river if it is (on average) four feet deep. ‚ÄîNassim Taleb\n\nWorking with averages may mask the underlying distribution. An outlier may significantly change the picture. This is important for anyone that works with data. A few extreme outliers may dominate."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#information-bias",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#information-bias",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "10. Information Bias",
    "text": "10. Information Bias\nThis is when additional information does not add any value to the action or decision you are taking. Extra information does not guarantee better decisions, and at times it may actually put you at a disadvantage in addition to wasting time and money. Observational studies may be more prone to this type of bias, mainly because they rely on self-reporting data collection, although introducing randomness in interventional studies may reduce that. Missing data can be another reason for introducing information bias2."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#the-law-of-small-numbers",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#the-law-of-small-numbers",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "11. The Law of Small Numbers",
    "text": "11. The Law of Small Numbers\nThis law refers to incorrect reasoning that a small sample drawn from a population resembles the overall population. This is particularly important in statistical analysis. In his book ‚ÄúThinking, Fast and Slow‚Äù (see Kahneman 2011), the Nobel prize winner Daniel Kahneman points out that even professionals and experts sometimes fall into this fallacy.\nSuppose you have a bag of 1000 marbles, of which 500 are red, and the other 500 marbles are black. Without looking, you draw three marbles, and all turn out to be black. You may infer that all marbles in the bag are black. In this scenario, you‚Äôve fallen into the fallacy of the law of small numbers.\nThe best way to counter this fallacy, as you may have guessed by now, is to use the Law of Large Numbers to have a sample size that resembles the overall population."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#ambiguity-aversion-risk-vs-uncertainty",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#ambiguity-aversion-risk-vs-uncertainty",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "12. Ambiguity Aversion (Risk vs Uncertainty)",
    "text": "12. Ambiguity Aversion (Risk vs Uncertainty)\nSuppose we have two bags. Bag A has 50 black and another 50 red marbles. Bag B also has 100 marbles (red and black), but we don‚Äôt know how many are for each color. Now, if you pull a red marble from a bag (of course without taking a look!!), you will win $100. So, do you choose bag A or bag B?\nThe majority will go with bag A. Now, let‚Äôs repeat the experiment, and this time if you pull a black marble from a bag, you will win $100. In this case, the majority will pick bag A too. This illogical result is known as Ellsberg Paradox, stating that people prefer known probabilities over unknown (ambiguous) ones (see Dobelli 2013). We should be aware of the difference between risks and uncertainties. These two terms are used interchangeably. However, there is a significant difference between the two.\n\n\n\n\n\n\nNote\n\n\n\nüëâ Risks are known probabilities, whereas uncertainties are unknown probabilities (ambiguous).\n\n\nAlthough difficult, tolerating ambiguity/uncertainty may help us to handle this fallacy better."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#planning-fallacy",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#planning-fallacy",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "13. Planning Fallacy",
    "text": "13. Planning Fallacy\nHave you wondered why you always underestimate the amount of time it takes you to finish a task or a project? You may fall into the planning fallacy. This cognitive error says that our planning is usually very ambitious. The two main reasons behind this fallacy are 1) wishful thinking and 2) neglecting the external influences. One way we can better plan is to have a premortem session that you can go over similar projects or consider outside influences beforehand."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#d√©formation-professionnelle",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#d√©formation-professionnelle",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "14. D√©formation Professionnelle",
    "text": "14. D√©formation Professionnelle\n\nIf your only tool is a hammer, all your problems will be nails. ‚ÄîMark Twain\n\nThis fallacy is in place when you view things from one‚Äôs own profession instead of a broader perspective. This may impact anyone who learns something new and uses that in every situation. Let‚Äôs say you just learned about a new machine learning model. You may use it in cases that better models are available. I actually did the same. During my studies, I learned about support vector machine (SVM) (a binary classifier). Whenever I had a classification problem, I would initially think of using SVM, even in multiclass classification for which SVM may not be the best option to start for."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#exponential-growth-bias",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#exponential-growth-bias",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "15. Exponential Growth Bias",
    "text": "15. Exponential Growth Bias\nUnlike linear growths, exponential growths are not intuitive to understand. Consider the following two options:\n\nYou will get $1,000 every day for the next 30 days.\nYou will get a cent on the first day, two cents on the second day, and the amount gets doubled every day for the next 30 days.\n\nWhich one do you choose? Option 1 will earn you $30,000 at the end, whereas Option 2 will get you over $10.7 million (you can calculate it yourself using x(t) = x_{0}(1 + \\frac{r}{100})^{t} where x_0=0.01 (1 cent), r=100\\% (doubling), t=30).\nWe can use a logarithmic scale to transform the original data for a better understanding."
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html",
    "href": "blog/deploy-postgresql-db-heroku/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: June 21, 2021 11:09 PM Last Updated: June 27, 2021 Last updated: September 7, 2022 10:13 PM Reading Time: 6 min read Show on Homepage?: No Tags: Database, Guide to, Python id: 13\n\nüëâ This article is also published on¬†Towards Data Science blog.\n\n\nTable of Contents"
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html#one-line-summary-of-related-technologies",
    "href": "blog/deploy-postgresql-db-heroku/index.html#one-line-summary-of-related-technologies",
    "title": "Step-by-Step Deployment of a Free PostgreSQL Database And Data Ingestion",
    "section": "One-line summary of related technologies",
    "text": "One-line summary of related technologies\nPostgreSQL: a free and open-source object-relational database management system that emphasizes extensibility and SQL compliance.\nHeroku: a platform as a service (PaaS) suitable for quick deployments with minimal needed DevOps experience.\nSQLAlchemy: a Python SQL library and Object Relational Mapper (ORM) to interact with databases.\nPandas: An open-source Python library for data analysis and manipulation."
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html#sign-up-to-heroku-deploy-your-first-postgresql-database",
    "href": "blog/deploy-postgresql-db-heroku/index.html#sign-up-to-heroku-deploy-your-first-postgresql-database",
    "title": "Step-by-Step Deployment of a Free PostgreSQL Database And Data Ingestion",
    "section": "1. Sign up to Heroku & Deploy Your First PostgreSQL Database",
    "text": "1. Sign up to Heroku & Deploy Your First PostgreSQL Database\nYou can signup for free to Heroku. After signing up and logging into your account, you will be directed to the Heroku Dashboard. Then, you can follow the instructions in the following clip to create a new app and add a PostgreSQL database.\n\n\nThe free plan allows you to have a maximum of 20,000 rows of data and up to 20 connections to the database. This plan is usually enough for a small personal project.\n\n\n\n\n\n\nDanger\n\n\n\nIn the free plan, the database credentials will occasionally change since Heroku rotates credentials periodically and sometimes perform maintenances.\n\n\nTo address the issue of occasional changes in the database credentials, we can use Heroku CLI to retrieve the database URL dynamically. But first, let‚Äôs go over the procedure for logging in to your account via Heroku CLI."
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html#access-your-heroku-account-using-token",
    "href": "blog/deploy-postgresql-db-heroku/index.html#access-your-heroku-account-using-token",
    "title": "Step-by-Step Deployment of a Free PostgreSQL Database And Data Ingestion",
    "section": "2. Access your Heroku account using Token",
    "text": "2. Access your Heroku account using Token\nWhat‚Äôs covered in this section is applicable in general for working with any Heroku applications through Heroku CLI.\n\n2.1. Generate a Heroku API Token\nYou can generate the token in the following two ways:\n2.1.1. Heroku account (browser)\nGo to Account settings ‚Üí Applications. Under the Authorizations section, click on Create authorization. You have to give a description in the opened window and set the expiry time or just set no expiry for the token (by leaving the box blank).\n\n\n\nCreate a Heroku API token from Heroku dashboard\n\n\n2.1.2. Heroku CLI (terminal)\nAfter installing the Heroku CLI, the first time you try to use a command that requires access to your account, you will be prompted to log in to your Heroku account on your browser. Once you‚Äôre logged in, you can do almost anything through the Heroku API. For example, we can create a token by running the following:\n$heroku authorization:create\nThe above command will generate a long-lived token for you. The first time you run the above command, you will be prompted to log in to your account in a browser. Once you successfully log into your account, you can get back to the terminal and see the generated token, as shown below.\n\n\n\nGenerate a Heroku API token via Heroku CLI\n\n\n\n\n2.2. Store your Heroku token in your environment\nNow that you have your Heroku API token, you need to set it in your terminal/environment as HEROKU_API_KEY. You can achieve this by running the following in your terminal:\nexport HEROKU_API_KEY=<your_token>\n\n\n\n\n\n\nTip\n\n\n\nA variable set in the shell terminal will only be available in the terminal from which you ran and will die after closing it. Instead, you can put above command in your ~/.bash or ~/.bashrc file so that the variable will be available in any new terminal you open. This way, you don‚Äôt need to worry about setting this variable again!\n\n\nOnce you have HEROKU_API_KEY variable set in your terminal, you no longer need to use the web-based authentication or username and password to log in. This is particularly important if you want to use Heroku CLI as a part of an automation process or CI/CD. This way, you don‚Äôt need to log in each time and use the token in any different terminals.\n\n\n2.3 Retrieve Heroku PostgreSQL Database URL\nYou can get the database URL by running the following command:\n$heroku config:get DATABASE_URL --app <your-app-name>\nThis will output the database URL in the format of\npostgres://<db_user>:<db_password>@<db_host>/<db_name>\nWe can use Python‚Äôs standard library subprocess to run above command and retrieve the database credentials. This way we will have all our codes in Python!\nimport subprocess\nheroku_app_name = \"your-app-name\"\nraw_db_url = subprocess.run(\n    [\"heroku\", \"config:get\", \"DATABASE_URL\", \"--app\", heroku_app_name],\n    capture_output=True  # capture_output arg is added in Python 3.7\n).stdout \n\n\n\n\n\n\nImportant\n\n\n\nYour Python (iPython) terminal/environment should have HEROKU_API_KEY set. You can verify that by running os.environ[\"HEROKU_API_KEY\"] and verifying the token in the output."
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html#create-sqlalchemy-engine",
    "href": "blog/deploy-postgresql-db-heroku/index.html#create-sqlalchemy-engine",
    "title": "Step-by-Step Deployment of a Free PostgreSQL Database And Data Ingestion",
    "section": "Create SQLAlchemy Engine",
    "text": "Create SQLAlchemy Engine\nBefore we ingest data to a table in the deployed PostgreSQL database using Pandas, we have to create an SQLAlchemy engine that will be passed to the Pandas method. The SQLAlchemy engine/connection can be created using the following code snippet:\nimport subprocess\nfrom sqlalchemy.engine.create import create_engine\n\n# Get the Database URL using Heroku CLI\n# -------------------------------------\n# Running the following from Python: $heroku config:get DATABASE_URL --app your-app-name\nheroku_app_name = \"your-app-name\"\n\n# Assumption: HEROKU_API_KEY is set in your terminal\n# You can confirm that it's set by running the following python command os.environ[\"HEROKU_API_KEY\"]\nraw_db_url = subprocess.run(\n    [\"heroku\", \"config:get\", \"DATABASE_URL\", \"--app\", heroku_app_name],\n    capture_output=True  # capture_output arg is added in Python 3.7\n).stdout \n\n# Convert binary string to a regular string & remove the newline character\ndb_url = raw_db_url.decode(\"ascii\").strip()\n\n# Convert \"postgres://<db_address>\"  --> \"postgresql+psycopg2://<db_address>\" needed for SQLAlchemy\nfinal_db_url = \"postgresql+psycopg2://\" + db_url.lstrip(\"postgres://\")  # lstrip() is more suitable here than replace() function since we only want to replace postgres at the start!\n\n# Create SQLAlchemy engine\n# ------------------------\nengine = create_engine(final_db_url)\nAs you may note in the above, some string manipulation is required before creating the SQLAlchemy engine."
  },
  {
    "objectID": "blog/deploy-postgresql-db-heroku/index.html#ingest-data-using-pandas-sqlalchemy",
    "href": "blog/deploy-postgresql-db-heroku/index.html#ingest-data-using-pandas-sqlalchemy",
    "title": "Step-by-Step Deployment of a Free PostgreSQL Database And Data Ingestion",
    "section": "Ingest Data using Pandas & SQLAlchemy",
    "text": "Ingest Data using Pandas & SQLAlchemy\nWe can ingest the data into a table by simply using pandas to_sql() function and passing the SQLAlchemy engine/connection object to it.\nimport pandas as pd\nfrom sqlalchemy.types import Integer, DateTime\n\nDATA_URL = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/latest/owid-covid-latest.csv\"\ndf = pd.read_csv(DATA_URL)\n\ndf.to_sql(\n    \"covid19\",  # table name\n    con=engine,\n    if_exists='replace',\n    index=False,  # In order to avoid writing DataFrame index as a column\n    dtype={\n        \"last_updated_date\": DateTime(),\n        \"total_cases\": Integer(),\n        \"new_cases\": Integer()\n    }\n)\nIn the above example, the data type of few columns is specified. You can determine the dtype of columns by passing a dictionary in which keys should be the column names and the values should be the SQLAlchemy types. For all available dtypes, you can check SQLAlchemy documentation for the data types it supports."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory data analysis (EDA) is an important step in any data science project. We always try to get a glance of our data by computing descriptive statistics of our dataset. If you are like me, the first function you call might be Pandas dataframe.describe() to obtain descriptive statistics. While such analysis is important, we often underestimate the importance of choosing the correct sample statistics/metrics/estimates.\nIn this post, we will go over several metrics that you can use in your data science projects. In particular, we are going to cover several estimates of location and variability and their robustness (sensitiveness to outliers).\nThe following common metrics/estimates are covered in this article:\nFor each metric, we will cover:\nA note before we start: data scientists and business analysts usually refer to values calculated from the data as a metric, whereas statisticians use the term estimates for such values(see Bruce and Bruce 2017)."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mean",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mean",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Mean",
    "text": "Mean\nThe arithmetic mean, or simply mean or average is probably the most popular estimate of location. There different variants of mean, such as weighted mean or trimmed/truncated mean. You can see how they can be computed below.\n\n\\begin{matrix}\n    \\text{Mean} & {= \\bar{x} = \\frac{\\sum\\limits_{i}^{n}x_{i}}{n}\\quad\\quad\\quad\\quad} & {(1.1)} \\\\\n    \\text{Weighted Mean} & {= {\\bar{x}}_{w} = \\frac{\\sum\\limits_{i = 1}^{n}w_{i}x_{i}}{\\sum\\limits_{i}^{n}w_{i}}} & {(1.2)} \\\\\n    \\text{Truncated Mean} & {= {\\bar{x}}_{\\text{tr}} = \\frac{\\sum\\limits_{i = p + 1}^{n - p}x_{i}}{n - 2p}} & {(1.3)}\n\\end{matrix}\n\nn denotes the total number of observations (rows).\nWeighted mean (equation 1.2) is a variant of mean that can be used in situations where the sample data does not represent different groups in a dataset. By assigning a larger weight to groups that are under-represented, the computed weighted mean will more accurately represent all groups in our dataset.\n\n\n\n\n\n\nImportant\n\n\n\nExtreme values can easily influence both the mean and weighted mean since neither one is a robust metric!\n\n\n\n\n\n\n\n\nüí° Robust estimate: A metric that is not sensitive to extreme values (outliers).\n\n\n\n\nAnother variant of mean is the trimmed mean (eq. 1.3) that is a robust estimate. This metric is used in calculating the final score in many sports where a panel of judges will each give a score. Then the lowest and the highest scores are dropped and the mean of the remaining scores are computed as a part of the final score1. One such example is in the international diving score system.\n\n\n\n\n\n\nüí° In statistics, \\bar{\\mathbf{x}} refers to a sample mean, whereas \\mu refers to the population mean.\n\n\n\n\nA Use Case for the Weighted Mean\nIf you want to buy a smartphone or a smartwatch or any gadget where there are many options, you can use the following method to choose among various options available for a gadget.\nLet‚Äôs assume you want to buy a smartphone, and the following features are important to you: 1) battery life, 2) camera quality, 3) price, and 4) the phone design. Then, you give the following weights to each one:\n\nList of features and their corresponding weights\n\n\n\n\n\n\nFEATURE\nWEIGHT\n\n\n\n\nBattery life\n0.15\n\n\nCamera quality\n0.30\n\n\nPrice\n0.25\n\n\nPhone design\n0.30\n\n\n\n\n\nLet‚Äôs say you have two options an iPhone and Google‚Äôs Pixel. You can give each feature a score of some value between 1 and 10 (1 being the worst and 10 being the best). After going over some reviews, you may give the following scores to the features of each phone.\n\n\n\nTable 2: Scores given to iPhone and Pixel for each score\n\n\nSo, which phone is better for you?\n\n\\begin{matrix}\n    \\text{iPhone score} & {= 0.15 \\times 6 + 0.3 \\times 9 + 0.25 \\times 1 + 0.3 \\times 9 = 6.55} \\\\\n    \\text{Google Pixel score} & {= 0.15 \\times 5 + 0.3 \\times 9.5 + 0.25 \\times 8 + 0.3 \\times 5 = 7.1} \\\\\n\\end{matrix}\n\nAnd based on your feature preferences, the Google Pixel might be the better option for you!"
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#median",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#median",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Median",
    "text": "Median\nMedian is the middle of a sorted list, and it‚Äôs a robust estimate. For an ordered sequence x_1,‚ÄÜx_2,‚ÄÜ...,‚ÄÜx_n, the median is computed as follows:\n\n\\begin{matrix}\n    {\\text{if~}n\\text{~is~odd}\\quad} & \\left. {}\\longrightarrow\\quad\\text{Median} = x_{\\frac{n + 1}{2}} \\right. \\\\\n    {\\text{if~}n\\text{~is~even}\\quad} & \\left. {}\\longrightarrow\\quad\\text{Median} = \\frac{1}{2}(x_{\\frac{n}{2}} + x_{\\frac{n + 1}{2}}) \\right. \\\\\n\\end{matrix}\n\nAnalogous to the weighted mean, we can also have the weighted median that can be computed as follows for an ordered sequence x_1,‚ÄÜx_2,‚ÄÜ...,‚ÄÜx_n with weights w_1,‚ÄÜw_2,‚ÄÜ‚Ä¶,‚ÄÜw_n where w_i>0.\n\n\\begin{matrix}\n    & {\\text{Weighted~Median} = x_{k}} \\\\\n    & {\\text{where}\\quad\\sum\\limits_{i = 1}^{n}w_{i} = 1\\quad\\text{and}\\quad\\sum\\limits_{i = k + 1}^{n}w_{i} \\leq \\frac{1}{2}\\quad\\text{and}\\quad\\sum\\limits_{i = 1}^{k - 1}w_{i} \\leq \\frac{1}{2}} \\\\\n\\end{matrix}"
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mode",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mode",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Mode",
    "text": "Mode\nThe mode is the value that appears most often in the data and is typically used for categorical data, and less for numeric data (see Bruce and Bruce 2017)."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#python-implementation",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#python-implementation",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Python Implementation",
    "text": "Python Implementation\nLet‚Äôs first import all necessary Python libraries and generate our dataset.\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport robustats\n\ndf = pd.DataFrame({\n    \"data\": [2, 1, 2, 3, 2, 2, 3, 20],\n    \"weights\": [1, 0.5, 1, 1, 1, 1, 1, 0.5] # Not necessarily add up to 1!!\n})\ndata, weights = df[\"data\"], df[\"weights\"]\n\nYou can use NumPy‚Äôs average() function to calculate the mean and weighted mean (equations 1.1 & 1.2). For computing truncated mean, you can use trim_mean() from the SciPy stats module. A common choice for truncating the top and bottom of the data is 10%(see Bruce and Bruce 2017).\nYou can use NumPy‚Äôs [median()](https://numpy.org/doc/stable/reference/generated/numpy.median.html) function to calculate the median. For computing the weighted median, you can use weighted_median() from the robustats Python library (you can install it using pip install robustats)2. Robustats is a high-performance Python library to compute robust statistical estimators implemented in C.\nFor computing the mode, you can either use the mode() function either from the robustats library that is particularly useful on large datasets or from scipy.stats module.\nmean = np.average(data) # You can use Pandas dataframe.mean()\nweighted_mean = np.average(data, weights=weights)\ntruncated_mean = stats.trim_mean(data, proportiontocut=0.1)\nmedian = np.median(data) # You can use Pandas dataframe.median()\nweighted_median = robustats.weighted_median(x=data, weights=weights)\nmode = stats.mode(data)  # You can also use robustats.mode() on larger datasets\n\nprint(\"Mean: \", mean.round(3))\nprint(\"Weighted Mean: \", weighted_mean.round(3))\nprint(\"Truncated Mean: \", truncated_mean.round(3))\nprint(\"Median: \", median)\nprint(\"Weighted Median: \", weighted_median)\nprint(\"Mode: \", mode)\n>>> Mean:  4.375\n>>> Weighted Mean:  3.5\n>>> Truncated Mean:  4.375\n>>> Median:  2.0\n>>> Weighted Median:  2.0\n>>> Mode:  ModeResult(mode=array([2]), count=array([4]))\nNow, let‚Äôs see if we just remove 20 from our data, how that will impact our mean.\nmean = np.average(data[:-1]) # Remove the last data point (20)\nprint(\"Mean: \", mean.round(3))\n\n>>> Mean:  2.143\nYou can see how the last data point (20) impacted the mean (4.375 vs 2.143). There can be many situations that we may end up with some outliers that should be cleaned from our datasets like faulty measurements that are in orders of magnitude away from other data points."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mean-absolute-deviation",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#mean-absolute-deviation",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Mean Absolute Deviation",
    "text": "Mean Absolute Deviation\nOne way to get this estimate is to calculate the difference between the largest and the lowest value to get the range. However, the range is, by definition, very sensitive to the two extreme values. Another option is the mean absolute deviation that is the average of the sum of all absolute deviation from the mean, as can be seen in the below formula:\n\n\\text{Mean~absolute~deviation} = \\frac{\\sum\\limits_{i = 1}^{n}\\mid x_{i} - \\bar{x}\\mid}{n}\n\nOne reason why the mean absolute deviation receives less attention is since mathematically it‚Äôs preferable not to work with absolute values if there are other desirable options such as squared values available (for instance, x^2 is differentiable everywhere while the derivative of ‚ÄÖ|x| is not defined at x=0)."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#variance-standard-deviation",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#variance-standard-deviation",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Variance & Standard Deviation",
    "text": "Variance & Standard Deviation\nThe variance and standard deviation are much more popular statistics than the mean absolute deviation to estimate the data dispersion.\n\n\\begin{matrix}\n\\text{Variance} & {= s^{2} = \\frac{\\sum\\limits_{i = 1}^{n}(x_{i} - \\bar{x})^{2}}{n - 1}} \\\\\n& \\\\\n\\text{Standard Deviation} & {= s = \\sqrt{\\text{Variance}}} \\\\\n\\end{matrix}\n\n\n\n\n\n\n\nImportant\n\n\n\nIn statistics, s is used to refer to a sample standard deviation, whereas \\sigma refers to the population standard deviation.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe variance is actually the average of the squared deviations from the mean.\n\n\nAs can be noted from the formula, the standard deviation is on the same scale as the original data making it an easier metric to interpret than the variance. Analogous to the trimmed mean, we can also compute the trimmed/truncated standard deviation that is less sensitive to outliers.\nA good way of remembering some of the above estimates of variability is to link them to other metrics or distances that share a similar formulation (see Bruce and Bruce 2017). For instance,\n\n\n\n\n\n\nüí° Variance \\equiv Mean Squared Error (MSE) (aka Mean Squared Deviation MSD)\n\n\n\n\n\n\n\n\n\nüí° Standard deviation \\equiv L2-norm, Euclidean norm\n\n\n\n\n\n\n\n\n\nüí° Mean absolute deviation \\equiv L1-norm, Manhattan norm, Taxicab norm"
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#median-absolute-deviation-mad",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#median-absolute-deviation-mad",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Median Absolute Deviation (MAD)",
    "text": "Median Absolute Deviation (MAD)\nLike the arithmetic mean, none of the estimates of variability (variance, standard deviation, mean absolute deviation) is robust to outliers. Instead, we can use the median absolute deviation from the median to check how our data is spread out in the presence of outliers. The median absolute deviation is a robust estimator, just like the median.\n\n\\begin{matrix}\n& {\\text{Median absolute deviation} = \\text{Median}(\\mid x_{1} - m\\mid,\\mid x_{2} - m\\mid,...,\\mid x_{n} - m\\mid)} \\\\\n& {\\quad\\quad\\text{where }m\\text{ is the median}} \\\\\n\\end{matrix}"
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#percentiles",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#percentiles",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Percentiles",
    "text": "Percentiles\nPercentiles (or quantiles) is another measure of the data dispersion that is based on order statistics (statistics based on sorted data). P-th percentile is the least percentage of the values that are lower than or equal to P percent.\n\n\n\n\n\n\nüí° The median is the 50th percentile (0.5 quantile, or Q2).\n\n\n\n\n\n\n\n\n\nüí° The percentile is technically a weighted average(see Bruce and Bruce 2017).\n\n\n\n25th (Q1) and 75th (Q3) percentiles are particularly interesting since their difference (Q3 ‚Äì Q1) shows the middle 50% of the data. The difference is known as the interquartile range (IQR) (IQR=Q3-Q1). Percentiles are used to visualize data distribution using boxplots.\nA nice article about boxplots is available on Towards Data Science blog."
  },
  {
    "objectID": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#python-implementation-1",
    "href": "blog/guide-to-estimates-in-exploratory-data-analysis/index.html#python-implementation-1",
    "title": "A Guide to Metrics (Estimates) in Exploratory Data Analysis",
    "section": "Python Implementation",
    "text": "Python Implementation\nYou can use NumPy‚Äôs var() and std() function to calculate the variance and standard deviation, respectively. On the other hand, to calculate the mean absolute deviation, you can use Pandas mad() function. For computing the trimmed standard deviation, you can use SciPy‚Äôs tstd() from the stats module. You can use Pandas boxplot() to quickly visualize a boxplot of the data.\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nvariance = np.var(data)\nstandard_deviation = np.std(data)  # df[\"Population\"].std()\nmean_absolute_deviation = df[\"data\"].mad()\ntrimmed_standard_deviation = stats.tstd(data)\nmedian_absolute_deviation = stats.median_abs_deviation(data, scale=\"normal\")  # stats.median_absolute_deviation() is deprecated\n\n# Percentile\nQ1 = np.quantile(data, q=0.25)  # Can also use dataframe.quantile(0.25)\nQ3 = np.quantile(data, q=0.75)  # Can also use dataframe.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(\"Variance: \", variance.round(3))\nprint(\"Standard Deviation: \", standard_deviation.round(3))\nprint(\"Mean Absolute Deviation: \", mean_absolute_deviation.round(3))\nprint(\"Trimmed Standard Deviation: \", trimmed_standard_deviation.round(3))\nprint(\"Median Absolute Deviation: \", median_absolute_deviation.round(3))\nprint(\"Interquantile Range (IQR): \", IQR)\n>>> Variance:  35.234\n>>> Standard Deviation:  5.936\n>>> Mean Absolute Deviation:  3.906\n>>> Trimmed Standard Deviation:  6.346\n>>> Median Absolute Deviation:  0.741\n>>> Interquantile Range (IQR):  1.0\n\n\n\n\nTable 3: A list of all metrics/estimates"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: March 8, 2021 11:38 AM Last Updated: January 29, 2021 Last updated: September 7, 2022 10:14 PM Reading Time: 8 min read Show on Homepage?: No Tags: Conda, Package Management, Poetry, Programming, Python id: 11\n\nüëâ This article is also published on Towards Data Science blog.\n\n\nTable of Contents\n\nIf you work on multiple Python projects at different development stages, you probably have different environments on your system. There are various tools for creating an isolated environment and install the libraries you need for your project. This post discusses different available technologies for Python packaging, environment, and dependencies management systems. Then, we will go over an ideal setup (of course, in my opinion üôÇ) suitable for most Python projects using conda and Poetry.\nüëâ In this post, the words library and package are used interchangeably, and they both refer to the Python package."
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#a-quick-note-on-package-repositories",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#a-quick-note-on-package-repositories",
    "title": "Essi Alizadeh",
    "section": "A Quick Note on Package Repositories",
    "text": "A Quick Note on Package Repositories\nThe most popular Python package repository is the Python Package Index (PyPI), a public repository for many Python libraries. You can install packages from PyPI by running pip install package_name. Python libraries can also be packaged using conda, and a popular host for conda packages is Anaconda. You can install conda packages by running conda install package_name in your conda environment."
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#issues-with-conda",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#issues-with-conda",
    "title": "Essi Alizadeh",
    "section": "Issues with conda",
    "text": "Issues with conda\nI think conda tries to do too much. After several years of using conda, here are few of my observations on conda as a package and dependency management:\n\nPerformance issues\nMy main problem with conda is its performance issues. Creating a new environment or even updating an old one may sometimes take a long time, especially if you have many packages. This is probably because conda tries to resolve the dependencies. There were few times that it took more than 30 minutes (yes, 30 minutes, not 30 seconds!) to create an environment. I initially thought that there is a connection issue or problems with connecting to the package repositories.\n\n\nDependency resolver issues\nConda may not even resolve the dependency issues. Since we cannot see the dependencies of specific conda packages (unlike Poetry), it may not be easy to resolve those issues.\n\n\nPython packaging\nAnother issue with conda is when you want to build a conda package for your library and publish it. It‚Äôs not trivial (at least for me) since you would need several configuration files (like meta.yml, setup.py, etc.). You may have dependency issues too. You can find more information on how to build a conda package here."
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#pyproject.toml-python-configuration-file",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#pyproject.toml-python-configuration-file",
    "title": "Essi Alizadeh",
    "section": "pyproject.toml: Python Configuration file",
    "text": "pyproject.toml: Python Configuration file\npyproject.toml file is a new Python configuration file defined in PEP518 to store build system requirements, dependencies, and many other configurations. You can even replace setup.cfg and setup.py files in most scenarios. You can save most configurations related to specific python packages like pytest, coverage, bumpversion, Black code styling, and many more in a single pyproject.toml file. You previously had to either write those configurations in individual files or other configuration files like setup.cfg. However, pyproject.toml can include all of them and also all project package requirements too."
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-1-create-a-minimal-conda-environment",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-1-create-a-minimal-conda-environment",
    "title": "Essi Alizadeh",
    "section": "Step 1: Create a minimal conda environment",
    "text": "Step 1: Create a minimal conda environment\nYou can create a conda environment from the following YAML file by running conda env create -f environment.yaml. This will create a fresh conda environment that has Python 3.8. In a conda environment, you can pass a list of channels (the order is important) from which you want to install your packages. In addition to the default channel on Anaconda Cloud that is curated by Anaconda Inc., there are other channels that you can install packages. A popular channel is conda-forge that includes a community-led collection of packages. If you have a private conda channel, you can write it in the channels section.\nname: post\n\nchannels:\n  - default\n  - conda-forge\n\ndependencies:\n  - python=3.8"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-2-install-poetry-tool",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-2-install-poetry-tool",
    "title": "Essi Alizadeh",
    "section": "Step 2: Install Poetry tool",
    "text": "Step 2: Install Poetry tool\nYou can install Poetry as per their instruction here. The recommended way is to install Poetry using the following command for OSx, Linux, or WSL (Windows Subsystem Linux).\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\nNote: Installing Poetry using the preferred approach that is by the custom installer (the first approach that downloads get-poetry.py script) will install Poetry isolated from the rest of the system.\n‚ö†Ô∏è Although not recommended, there is also a pip version of Poetry that you can install (pip install poetry). The developers warn against using the pip version in the documentation since it might cause some conflicts with other packages in the environment. But, if our environment is basically empty (although some base packages are installed like pip when creating a conda environment), then it is probably fine to install it through pip!"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-3-configure-your-poetry",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-3-configure-your-poetry",
    "title": "Essi Alizadeh",
    "section": "Step 3: Configure your Poetry",
    "text": "Step 3: Configure your Poetry\nTo configure Poetry for a new project, Poetry makes it very easy to create a configuration file with all your desired settings. You can interactively create a pyproject.toml file by simply running poetry init. This will prompt few questions about the desired Python packages you want to install. You can press Enter to process with default options.\n\n\n\nInteractive configuration by running poetry init\n\n\nInteractive configuration by running poetry init\nAs you can see in the above screenshot, you can add some packages only for development dependencies. Initializing the Poetry for your project will create the pyproject.toml file that includes all configurations we defined during the setup. We have one main section for all dependencies (used in both production and development environments), but we also have a section that contains packages used mainly for development purposes like pytest, sphinx, etc. This is the other advantage over other dependency management tools. You only need one configuration file for both your production and development environments.\n[tool.poetry]\nname = \"my_package\"\nversion = \"0.0.1\"\ndescription = \"\"\nauthors = [\"ealizadeh <abc@edf.com>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\nrequests = \"^2.25.1\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^6.2.1\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\naddopts = \"-ra -q\"\ntestpaths = [\n    \"tests\",  # You should have a \"tests\" directory\n]"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-4-installing-dependencies",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#step-4-installing-dependencies",
    "title": "Essi Alizadeh",
    "section": "Step 4: Installing dependencies",
    "text": "Step 4: Installing dependencies\nOnce you have your dependencies and other configurations in a pyproject.toml file, you can install the dependencies by simply running\npoetry install\nThis will create a poetry.lock file. This file basically contains the exact versions of all the packages locking the project with those specific versions. You need to commit both the pyproject.toml file and poetry.lock file. I would strongly recommend you not to update the poetry.lock file manually. Let poetry do its magic!!"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#add-new-packages",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#add-new-packages",
    "title": "Essi Alizadeh",
    "section": "Add new packages",
    "text": "Add new packages\nIf you want to add (or remove) a package to your environment, I would highly recommend you to do so by using the following command\npoetry add package_name\nThis will automatically add the package name and version to your pyproject.toml file and updates the poetry.lock accordingly. poetry add takes care of all dependencies, and adds the package in the [tool.poetry.dependencies] section.\nIf you want to add a package to your development environment, you can simly pass a --dev option as below\npoetry add package_name --dev\nYou can specify a specific version of a package, or even adding a package through git+https or git+ssh (see here for more details)."
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#remove-packages",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#remove-packages",
    "title": "Essi Alizadeh",
    "section": "Remove packages",
    "text": "Remove packages\nYou can remove a package as following\npoetry remove package_to_remove"
  },
  {
    "objectID": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#show-package-dependencies",
    "href": "blog/guide-to-python-env-pkg-dependency-using-conda-poetry/index.html#show-package-dependencies",
    "title": "Essi Alizadeh",
    "section": "Show package dependencies",
    "text": "Show package dependencies\nIf you want to see a list of all installed packages in your environment, you can run the following command:\npoetry show\nNote that this will show the package dependencies too. It is sometimes helpful to see the dependencies of a Python package. Fortunately, you can do so using poetry show . For instance, we can see the list of dependencies of requests package in our environment using the following command:\npoetry show requests\n\n\n\nAll dependencies of requests package in the project\n\n\nAll dependencies of requests package in the project\nEven better, you can see all your project‚Äôs dependencies by just running:\npoetry show --tree\n\n\n\nA tree of all your project dependencies\n\n\nA tree of all your project dependencies\nFrom above figure, you can see that the blue-font package names (requests and pytest) are explicitly added to¬†pyproject.toml¬†file. Other libraries, in yellow, are their dependencies and do not need to be in your toml file.\nNote: You may use¬†pip freeze¬†(pip freeze > requirements.txt¬†if you want to output the result into a file) to output all installed packages in your environment, but that will be quite messy."
  },
  {
    "objectID": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html",
    "href": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: March 2, 2021 10:09 PM Last Updated: February 25, 2021 Last updated: September 7, 2022 10:14 PM Reading Time: 4 min read Show on Homepage?: No Tags: Poetry, Python, Software Development id: 12\n\nüëâ This article is also published on Towards Data Science blog.\n\n\nTable of Contents\n\nPackaging your Python library has never been easier now using Poetry. You may have a side project in Python that benefits others. You can publish it using Poetry. This post will show you how to build your own Python library and publish it on the most popular Python package repository PyPI.\nI will use one of my recent Python projects, PyPocket: a Python library (wrapper) for Pocket (previously known as Read It Later)."
  },
  {
    "objectID": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#step-1-build-your-package",
    "href": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#step-1-build-your-package",
    "title": "Essi Alizadeh",
    "section": "Step 1: Build your package",
    "text": "Step 1: Build your package\nOnce you have your Python package ready to be published, you first need to build your package using the following command from the directory that contains the pyproject.toml file:\npoetry build\n\n\n\nHow%20to%20Publish%20Your%20Python%20Package%20with%20just%202%20com%209bb65ba0dbab48d68759663e9c4ba7dd/Untitled.png\n\n\nThe above command will create two files in the¬†dist¬†(distribution) directory. A folder will be created if there is no¬†dist¬†folder.\nFirst, a source distribution (often known as¬†sdist) is created that is an archive of your package based on the current platform (.tar.gz¬†for Unix and¬†.zip¬†for Windows systems)[1].\nIn addition to sdist,¬†poetry build¬†creates a Python wheel (.whl) file. In a nutshell, a Python wheel is a ready-to-install format allowing you to skip the build stage, unlike the source distribution. A wheel filename is usually in the following format[2]:\n{pkg-name}-{pkg-version}(-{build}?)-{python-implementation}-{application binary interface}-{platform}.whl\nFrom the above figure, the package I built is called pypocket with version 0.2.0 in Python 3 that is not OS-specific (none ABI) and suitable to run on any processor architecture."
  },
  {
    "objectID": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#step-2-publish-your-package",
    "href": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#step-2-publish-your-package",
    "title": "Essi Alizadeh",
    "section": "Step 2: Publish your package",
    "text": "Step 2: Publish your package\nOnce the package is built, you can publish it on PyPI (or other package repositories).\n\n‚ö†Ô∏è Once you publish your package to PyPI, you will not be able to release an identical version (you can delete the package, but when trying to republish with the same version, you will get an error! I‚Äôve been there). Hence, it‚Äôs recommended to test any package before pushing it to PyPI.\n\n\nTest Your Package on TestPyPI\nIt‚Äôs a good idea to first publish your package using the TestPyPI framework. This way, if there is an issue with the published package, you can fix it and then publish it on PyPI. TestPyPI has an identical setup and user interface as PyPI, but it‚Äôs a separate framework. So, you need to create an account on TestPyPI too.\nNow, let‚Äôs publish our package on TestPyPI. First, add TestPyPI as an alternative package repository using the following command.\npoetry config repositories.testpypi https://test.pypi.org/legacy/\nYou can publish your package to TestPyPI as the following:\npoetry publish -r testpypi\n\n\n\nHow%20to%20Publish%20Your%20Python%20Package%20with%20just%202%20com%209bb65ba0dbab48d68759663e9c4ba7dd/Untitled%201.png\n\n\npoetry publish will ask for your username and password (you can also use a token instead, more on this later). Notice that both the source distribution (.tar.gz) and the Python wheel are uploaded. Once the package is published, you should see something like the following on TestPyPI.\n\n\n\nYou can check that here https://test.pypi.org/project/pypocket/\n\n\nYou can check that here https://test.pypi.org/project/pypocket/\nAs can be seen from the above screenshot, you can install the package pip install -i https://test.pypi.org/simple/ pypocket and test it.\n\n\nPublish Package on PyPI\nOnce you‚Äôre happy with your Python library, you can publish it on PyPI using the following command:\npoetry publish\nNote that by default, Poetry publishes a package to PyPI. Therefore, you do not need to do poetry config or pass any argument to poetry publish.\n\n\n\nA point on using API Token instead of username and password\nYou may notice that I‚Äôve used my username and password when trying to publish the package. I would recommend using a token instead. You may have multiple projects in your PyPI account, and you can generate an API token for each project (package). This is particularly important if you want to automate your python packaging not to use your username and password during automated deployments. Another advantage of using an API token is that you can easily remove a token and even generate multiple tokens for a project.\nYou can generate an API token by going to Account settings of your PyPI (or TestPyPI) account and then add an API token under the API tokens section. You will then be prompted to select a scope for your token (to use the token for a particular project or all your PyPI projects). The instruction to use the token will also be provided at this stage."
  },
  {
    "objectID": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#related-posts",
    "href": "blog/how-to-publish-your-python-package-with-just-2-commands/index.html#related-posts",
    "title": "Essi Alizadeh",
    "section": "Related posts",
    "text": "Related posts\nA Guide to Python Environment, Dependency and Package Management: Conda + Poetry - Personal Website & Blog\n\n‚¨ÖÔ∏è Previous Post\nBlog Posts\n‚û°Ô∏è Next Post\nBlog Posts"
  },
  {
    "objectID": "blog/introduction-to-dynamic-time-warping/index.html",
    "href": "blog/introduction-to-dynamic-time-warping/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Example 1\nIn this example, we have two sequences x and y¬†with different lengths.\n# Create two sequences\nx = [3, 1, 2, 2, 1]\ny = [2, 0, 0, 3, 3, 1, 0]\nWe cannot calculate the distance between x and y since they don‚Äôt have equal lengths (the code follows).\n\n\n\nExample 1: Euclidean distance between x and y (is it possible? ü§î )\n\n\nExample 1: Euclidean distance between x and y (is it possible? ü§î )\nfig, ax = plt.subplots(figsize=(14, 10))\n\n# Remove the border and axes ticks\nfig.patch.set_visible(False)\nax.axis('off')\n\nxx = [(i, x[i]) for i in np.arange(0, len(x))]\nyy = [(j, y[j]) for j in np.arange(0, len(y))]\n\nfor i, j in zip(xx, yy[:-2]):\n    ax.plot([i[0], j[0]], [i[1], j[1]], '--k', linewidth=4)\n\nax.plot(x, '-ro', label='x', linewidth=4, markersize=20, markerfacecolor='lightcoral', markeredgecolor='lightcoral')\nax.plot(y, '-bo', label='y', linewidth=4, markersize=20, markerfacecolor='skyblue', markeredgecolor='skyblue')\nax.set_title(\"Euclidean Distance!??\", fontsize=28, fontweight=\"bold\")\n\nfig.savefig(\"ex1_euclidean_distance.png\", **savefig_options)\n\nCompute DTW distance and warp¬†path\nMany Python packages calculate the DTW by just providing the sequences and the type of distance (usually Euclidean by default). Here, we use a popular Python implementation of DTW that is¬†FastDTW¬†which is an approximate DTW algorithm with lower time and memory complexities[2].\ndtw_distance, warp_path = fastdtw(x, y, dist=euclidean)\nNote that we are using SciPy‚Äôs distance function Euclidean that we imported earlier. For a better understanding of the warp path, let‚Äôs first compute the accumulated cost matrix and then visualize the path on a grid. The following code will plot a heatmap of the accumulated cost matrix.\ncost_matrix = compute_accumulated_cost_matrix(x, y)\nfig, ax = plt.subplots(figsize=(12, 8))\nax = sbn.heatmap(cost_matrix, annot=True, square=True, linewidths=0.1, cmap=\"YlGnBu\", ax=ax)\nax.invert_yaxis()\n\n# Get the warp path in x and y directions\npath_x = [p[0] for p in warp_path]\npath_y = [p[1] for p in warp_path]\n\n# Align the path from the center of each cell\npath_xx = [x+0.5 for x in path_x]\npath_yy = [y+0.5 for y in path_y]\n\nax.plot(path_xx, path_yy, color='blue', linewidth=3, alpha=0.2)\n\nfig.savefig(\"ex1_heatmap.png\", **savefig_options)\n\n\n\nExample 1: Accumulated cost matrix and warping path\n\n\nExample 1: Accumulated cost matrix and warping path\nThe color bar shows the cost of each point in the grid. As can be seen, the warp path (blue line) is going through the lowest cost on the grid. Let‚Äôs see the DTW distance and the warping path by printing these two variables.\nprint(\"DTW distance: \", dtw_distance)\nprint(\"Warp path: \", warp_path)\n\n>>> DTW distance:  6.0\n>>> Warp path:  [(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (4, 6)]\nThe warping path starts at point (0, 0) and ends at (4, 6) by 6 moves. Let‚Äôs also calculate the accumulated cost most using the functions we defined earlier and compare the values with the heatmap.\ncost_matrix = compute_accumulated_cost_matrix(x, y)\nprint(np.flipud(cost_matrix)) # Flipping the cost matrix for easier comparison with heatmap values!\n\n>>> [[32. 12. 10. 10.  6.]  \n     [23. 11.  6.  6.  5.]   \n     [19. 11.  5.  5.  9.]  \n     [19.  7.  4.  5.  8.]  \n     [19.  3.  6. 10.  4.]  \n     [10.  2.  6.  6.  3.]  \n     [ 1.  2.  2.  2.  3.]]\nThe cost matrix is printed above has similar values to the heatmap.\nNow let‚Äôs plot the two sequences and connect the mapping points. The code to plot the DTW distance between¬†x¬†and¬†y¬†is given below.\nfig, ax = plt.subplots(figsize=(14, 10))\n\n# Remove the border and axes ticks\nfig.patch.set_visible(False)\nax.axis('off')\n\nfor [map_x, map_y] in warp_path:\n    ax.plot([map_x, map_y], [x[map_x], y[map_y]], '--k', linewidth=4)\n\nax.plot(x, '-ro', label='x', linewidth=4, markersize=20, markerfacecolor='lightcoral', markeredgecolor='lightcoral')\nax.plot(y, '-bo', label='y', linewidth=4, markersize=20, markerfacecolor='skyblue', markeredgecolor='skyblue')\nax.set_title(\"DTW Distance\", fontsize=28, fontweight=\"bold\")\n\nfig.savefig(\"ex1_dtw_distance.png\", **savefig_options)\n\n\n\nExample 1: DTW distance between x and y\n\n\nExample 1: DTW distance between x and y\n\n\n\nExample 2\nIn this example, we will use two sinusoidal signals and see how they will be matched by calculating the DTW distance between them.\ntime1 = np.linspace(start=0, stop=1, num=50)\ntime2 = time1[0:40]\n\nx1 = 3 * np.sin(np.pi * time1) + 1.5 * np.sin(4*np.pi * time1)\nx2 = 3 * np.sin(np.pi * time2 + 0.5) + 1.5 * np.sin(4*np.pi * time2 + 0.5)\nJust like Example 1, let‚Äôs calculate the DTW distance and the warp path for x1 and x2 signals using FastDTW package.\ndistance, warp_path = fastdtw(x1, x2, dist=euclidean)\nfig, ax = plt.subplots(figsize=(16, 12))\n\n# Remove the border and axes ticks\nfig.patch.set_visible(False)\nax.axis('off')\n\nfor [map_x, map_y] in warp_path:\n    ax.plot([map_x, map_y], [x1[map_x], x2[map_y]], '-k')\n\nax.plot(x1, color='blue', marker='o', markersize=10, linewidth=5)\nax.plot(x2, color='red', marker='o', markersize=10, linewidth=5)\nax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n\nfig.savefig(\"ex2_dtw_distance.png\", **savefig_options)\n\n\n\nExample 2: DTW distance between x1 and x2\n\n\nExample 2: DTW distance between x1 and x2\nAs can be seen in above figure, the DTW distance between the two signals is particularly powerful when the signals have similar patterns. The extrema (maximum and minimum points) between the two signals are correctly mapped. Moreover, unlike Euclidean distance, we may see many-to-one mapping when DTW distance is used, particularly if the two signals have different lengths.\nYou may spot an issue with dynamic time warping from the figure above. Can you guess what it is?\nThe issue is around the head and tail of time-series that do not properly match. This is because the DTW algorithm cannot afford the warping invariance for at the endpoints. In short, the effect of this is that a small difference at the sequence endpoints will tend to contribute disproportionately to the estimated similarity[3].\n\n\n\nConclusion\nDTW is an algorithm to find an optimal alignment between two sequences and a useful distance metric to have in our toolbox. This technique is useful when we are working with two non-linear sequences, particularly if one sequence is a non-linear stretched/shrunk version of the other. The warping path is a combination of ‚Äúchess king‚Äù moves that starts from the head of two sequences and ends with their tails.\n\nüìì You can find the Jupyter notebook for this blog post on GitHub.\n\n\nThanks for reading üôè\nIf you liked this post, you can join my mailing list to receive similar posts. You can follow me on LinkedIn, GitHub, Twitter and Medium.\n**\nAnd finally, you can find my knowledge forest üå≤ (raw digital notes) at notes.ealizadeh.com.\n\n\nüì© Join my mailing list\nhttps://chilipepper.io/form/burning-darkcrimson-piquillo-fcdff002-5e7b-4d46-a75e-6290908a51f4\n\n\n\nReferences\n[1] Donald J. Berndt and James Clifford,¬†Using Dynamic Time Warping to Find Patterns in Time Series, 3rd International Conference on Knowledge Discovery and Data Mining\n[2] Salvador, S. and P. Chan,¬†FastDTW: Toward accurate dynamic time warping in linear time and space¬†(2007), Intelligent Data Analysis\n[3] Diego Furtado Silva,¬†et al.,¬†On the effect of endpoints on dynamic time warping¬†(2016), SIGKDD Workshop on Mining and Learning from Time Series\n\n\nUseful Links\nProgramatically understanding dynamic time warping (DTW)\nUnderstanding Dynamic Time Warping - The Databricks Blog\n\n‚¨ÖÔ∏è Previous Post\nBlog Posts\n‚û°Ô∏è Next Post\nBlog Posts\n\n\n\n\nCitationBibTeX citation:@online{essi,\n  author = {Essi},\n  editor = {},\n  url = {https://ealizadeh.com/blog/introduction-to-dynamic-time-warping},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. n.d. https://ealizadeh.com/blog/introduction-to-dynamic-time-warping."
  },
  {
    "objectID": "blog/mlxtend-library-for-data-science/index.html",
    "href": "blog/mlxtend-library-for-data-science/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Conclusion\nIn this post, we went over several MLxtend library functionalities, in particular, we talked about creating counterfactual instances for better model interpretability and plotting decision regions for classifiers, drawing PCA correlation circle, analyzing bias-variance tradeoff through decomposition, drawing a matrix of scatter plots of features with colored targets, and implementing the bootstrapping. The library is a nice addition to your data science toolbox, and I recommend giving this library a try.\n\nüìì You can find the Jupyter notebook for this blog post on GitHub.\n\nOne-Page Summary\nMLxtend%20A%20Python%20Library%20with%20Interesting%20Tools%20fo%20264ef2d64cce4a179cc8263b7c027dd3/mlxtend-one-page-summary.pdf\n\nThanks for reading üôè\nIf you liked this post, you can join my mailing list to receive similar posts. You can follow me on LinkedIn, GitHub, Twitter and Medium.\nAnd finally, you can find my knowledge forest üå≤ (raw digital notes) at notes.ealizadeh.com.\n\n\nüì© Join my mailing list\nhttps://chilipepper.io/form/burning-darkcrimson-piquillo-fcdff002-5e7b-4d46-a75e-6290908a51f4\n\n\n\nReferences\n[1] Sebastian Raschka,¬†MLxtend\n[2] Sebastian Raschka,¬†Create Counterfactual, MLxtend API documentation\n[3] S. Wachter¬†et al¬†(2018),¬†Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR, 31(2), Harvard Journal of Law & Technology\n[4] Wikipedia,¬†Bias‚Äìvariance tradeoff\n[5] Sebastian Raschka,¬†Bias-Variance Decomposition, MLxtend API documentation\n\n‚¨ÖÔ∏è Previous Post\nBlog Posts\n‚û°Ô∏è Next Post\nBlog Posts\n\n\n\n\nCitationBibTeX citation:@online{essi,\n  author = {Essi},\n  editor = {},\n  url = {https://ealizadeh.com/blog/mlxtend-library-for-data-science},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. n.d. https://ealizadeh.com/blog/mlxtend-library-for-data-science."
  },
  {
    "objectID": "blog/neural-prophet-library/index.html",
    "href": "blog/neural-prophet-library/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: March 9, 2021 9:57 AM Last Updated: May 30, 2021 Last updated: September 7, 2022 10:14 PM Reading Time: 4 min read Show on Homepage?: Yes Tags: Machine Learning, Neural-Network, Python Library, Time Series Analysis id: 7\n\nüëâ This article is also published on¬†Towards Data Science Blog.\n\n\nTable of Content\n\nNeuralProphet¬†is a python library for modeling time-series data based on neural networks. It‚Äôs built on top of¬†PyTorch¬†and is heavily inspired by¬†Facebook Prophet¬†and¬†AR-Net¬†libraries."
  },
  {
    "objectID": "blog/neural-prophet-library/index.html#neuralprophet-vs.-prophet",
    "href": "blog/neural-prophet-library/index.html#neuralprophet-vs.-prophet",
    "title": "Essi Alizadeh",
    "section": "NeuralProphet vs.¬†Prophet",
    "text": "NeuralProphet vs.¬†Prophet\nFrom the library name, you may ask what is the main difference between Facebook‚Äôs Prophet library and NeuralProphet. According to NeuralProphet‚Äôs¬†documentation, the added features are[1]:\n\nUsing PyTorch‚Äôs Gradient Descent optimization engine making the modeling process much faster than Prophet\nUsing AR-Net for modeling time-series autocorrelation (aka serial correlation)\nCustom losses and metrics\nHaving configurable non-linear layers of feed-forward neural networks,\netc."
  },
  {
    "objectID": "blog/neural-prophet-library/index.html#project-maintainers",
    "href": "blog/neural-prophet-library/index.html#project-maintainers",
    "title": "Essi Alizadeh",
    "section": "Project Maintainers",
    "text": "Project Maintainers\nBased on the project‚Äôs GitHub page, the main maintainer of this project is¬†Oskar Triebe¬†from Stanford University with collaboration from Facebook and Monash University."
  },
  {
    "objectID": "blog/neural-prophet-library/index.html#installation",
    "href": "blog/neural-prophet-library/index.html#installation",
    "title": "Essi Alizadeh",
    "section": "Installation",
    "text": "Installation\nThe project is in the beta phase, so I would advise you to be cautious if you want to use this library in a production environment.\nYou can install the package using¬†pip install neuralprophet. However, if you are going to use the package in a Jupyter Notebook environment, you should install their live version¬†pip install neuralprophet[live]. This will provide more features such as a live plot of train and validation loss using¬†plot_live_loss().\ngit clone https://github.com/ourownstory/neural_prophet\ncd neural_prophet\npip install .[live]\nI would recommend creating a fresh environment (a conda or venv) and installing the NeuralProphet package from the new environment letting the installer take care of all dependencies (it has Pandas, Jupyter Notebook, PyTorch as dependencies).\nNow that we have the package installed, let‚Äôs play!"
  },
  {
    "objectID": "blog/neural-prophet-library/index.html#implementation-with-a-case-study",
    "href": "blog/neural-prophet-library/index.html#implementation-with-a-case-study",
    "title": "Essi Alizadeh",
    "section": "Implementation with a Case Study",
    "text": "Implementation with a Case Study\nHere, I‚Äôm using the daily climate data in Delhi from 2013 to 2017 that I found on¬†Kaggle. First, let‚Äôs import the main packages.\nimport pandas as pd\nfrom neuralprophet import NeuralProphet\nThen, we can read the data into a Panda DataFrame. NeuralProphet object expects the time-series data to have a date column named ds and the time-series column value we want to predict as y.\n# Data is from https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data\ndf = pd.read_csv(\"./DailyDelhiClimateTrain.csv\", parse_dates=[\"date\"])\ndf = df[[\"date\", \"meantemp\"]]\ndf.rename(columns={\"date\": \"ds\", \"meantemp\": \"y\"}, inplace=True)\nNow let‚Äôs initialize the model. Below, I‚Äôve brought all default arguments defined for the NeuralProphet object, including additional information about some. These are the hyperparameters you can configure in the model. Of course, if you are planning to use the default variables, you can just do model = NeuralProphet().\n# model = NeuralProphet() if you're using default variables below.\nmodel = NeuralProphet(\n    growth=\"linear\",  # Determine trend types: 'linear', 'discontinuous', 'off'\n    changepoints=None, # list of dates that may include change points (None -> automatic )\n    n_changepoints=5,\n    changepoints_range=0.8,\n    trend_reg=0,\n    trend_reg_threshold=False,\n    yearly_seasonality=\"auto\",\n    weekly_seasonality=\"auto\",\n    daily_seasonality=\"auto\",\n    seasonality_mode=\"additive\",\n    seasonality_reg=0,\n    n_forecasts=1,\n    n_lags=0,\n    num_hidden_layers=0,\n    d_hidden=None,     # Dimension of hidden layers of AR-Net\n    ar_sparsity=None,  # Sparcity in the AR coefficients\n    learning_rate=None,\n    epochs=40,\n    loss_func=\"Huber\",\n    normalize=\"auto\",  # Type of normalization ('minmax', 'standardize', 'soft', 'off')\n    impute_missing=True,\n    log_level=None, # Determines the logging level of the logger object\n)\nAfter configuring the model and its hyperparameters, we need to train the model and make predictions. Let‚Äôs make up to a one-year prediction of the temperature.\nmetrics = model.fit(df, validate_each_epoch=True, freq=\"D\")\nfuture = model.make_future_dataframe(df, periods=365, n_historic_predictions=len(df))\nforecast = model.predict(future)\nYou can simply plot the forecast by calling model.plot(forecast) as following:\nfig, ax = plt.subplots(figsize=(14, 10))\nmodel.plot(forecast, xlabel=\"Date\", ylabel=\"Temp\", ax=ax)\nax.set_title(\"Mean Temperature in Delhi\", fontsize=28, fontweight=\"bold\")\nThe one-year forecast plot is shown below, where the time period between 2017-01-01 to 2018-01-01 is the prediction. As can be seen, the forecast plot resembles the historical time-series. It both captured the seasonality as well as the slow-growing linear trend.\n\n\n\nThe mean temperature in Delhi and the one-year prediction\n\n\nThe mean temperature in Delhi and the one-year prediction\nYou can plot the parameters by calling model.plot_parameters()\n\n\n\nModel Parameters\n\n\nModel Parameters\nThe model loss using Mean Absolute Error (MAE) is plotted below. You can also use the Smoothed L1-Loss function.\nfig, ax = plt.subplots(figsize=(14, 10))\nax.plot(metrics[\"MAE\"], 'ob', linewidth=6, label=\"Training Loss\")  \nax.plot(metrics[\"MAE_val\"], '-r', linewidth=2, label=\"Validation Loss\")\n\n# You can use metrics[\"SmoothL1Loss\"] and metrics[\"SmoothL1Loss_val\"] too.\n\n\n\nModel Loss using MAE\n\n\nModel Loss using MAE"
  },
  {
    "objectID": "blog/non-minimum-phase-systems/index.html",
    "href": "blog/non-minimum-phase-systems/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: March 9, 2021 10:40 AM Last Updated: November 20, 2021 Last updated: September 7, 2022 10:14 PM Reading Time: 7 min read Show on Homepage?: No Tags: Control Systems Theory, Engineering, Guide to, Mathematical Modeling, Transfer Function id: 3\n\nüëâ This article is also published on¬†Towards Data Science Blog.\n\n\nTable of Content\n\nHave you ever wondered why when you turn the hot water knob on in the shower, the water will be cold for a few seconds, and then it gets hot or vice versa?\nIn this article, we are going to answer this question by touching upon two concepts, namely the¬†Minimum Phase¬†(MP) systems, and¬†Transfer Functions."
  },
  {
    "objectID": "blog/non-minimum-phase-systems/index.html#what-is-the-transfer-function",
    "href": "blog/non-minimum-phase-systems/index.html#what-is-the-transfer-function",
    "title": "Essi Alizadeh",
    "section": "What is the Transfer Function?",
    "text": "What is the Transfer Function?\nA transfer function model describes an input-output relationship of a system using a ratio of polynomials. So, an input signal is given to a system to produce a controlled output (aka response). This type of modeling is different than using differential equations and state-space representations where the model dynamics is available.\n\nüí° A transfer function is another way of viewing a dynamical system but in the frequency domain by analyzing the system response for a given input signal.\n\nThe transfer function of a control system is the ratio of the Laplace transforms (\\mathscr{L}\\{\\}) of output signals over the Laplace transform of the input signal. In a nutshell, instead of analyzing a model in the time-domain using time-based differential equations, the goal here is to analyze the model in the frequency-domain using a transformation.\n\n\n\nA%20Guide%20to%20Non-Minimum%20Phase%20Systems%20b819ddfb4f604aa495d233069022760b/Untitled%201.png\n\n\n\nH(s)=\\frac{L\\{u(t)\\}}{L\\{y(t)\\}}=\\frac{U(s)}{Y(s)}\n\nSo, let‚Äôs say we have a system with¬†u(t)¬†and¬†y(t)¬†as our input and output signals. The transfer function can be calculated as shown above.\nThe roots of the numerator polynomial are called the model¬†zeros, and the roots of the denominator polynomials are called the model¬†poles. The zeros affect the input to a system, and the poles affect the system response and its stability. For zero-pole analysis, we should use the *s*‚àíplane¬†that is a complex plane on which Laplace transforms are graphed[1].\n\nüëâ In this article, we are only interested in the initial response of the system in order to answer the question of why the shower first gets cold before it gets hot? Since the¬†initial system response¬†is closely related to system zeros as noted earlier, therefore, we will not talk about the poles (perhaps that can be a topic for another article)."
  },
  {
    "objectID": "blog/non-minimum-phase-systems/index.html#what-are-non-minimum-phase-systems",
    "href": "blog/non-minimum-phase-systems/index.html#what-are-non-minimum-phase-systems",
    "title": "Essi Alizadeh",
    "section": "What are Non-Minimum Phase Systems?",
    "text": "What are Non-Minimum Phase Systems?\nNow that we are familiar with an NMP system, let‚Äôs define the system formally:\n\nüëâ Non-minimum Phase systems are causal and stable systems whose inverses are causal but unstable[2].\n\nHaving a delay in our system or a model zero on the right half of the¬†*s*‚àíplane¬†(aka Right-Half Plane or RHP) may lead to a non-minimum phase system.\nNote that there is only one minimum phase system for a given magnitude response, but there is an infinite number of NMP systems. This is the reason why we do not hear a term like the maximum phase system. More detail about non-minimum phase systems with mathematical description is available in [3]."
  },
  {
    "objectID": "blog/sdv-library-for-modeling-datasets/index.html",
    "href": "blog/sdv-library-for-modeling-datasets/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Created: March 4, 2021 7:28 PM Last Updated: February 25, 2021 Last updated: September 7, 2022 10:13 PM Reading Time: 6 min read Show on Homepage?: No Tags: Data Modeling, Data Science, Machine Learning, Python Library, Time Series Analysis id: 4\n\n\n\nhttps://ealizadeh.com/wp-content/uploads/2020/11/BP04_SDV_featured_image.png\n\n\n\nüëâ This article is also published on Towards Data Science blog.\n\nIn data science, you usually need a realistic dataset to test your proof of concept. Creating fake data that captures the behavior of the actual data may sometimes be a rather tricky task. Several python packages try to achieve this task. Few popular python packages are¬†Faker,¬†Mimesis. However, there are mostly generating simple data like generating names, addresses, emails,¬†etc.\nTo create data that captures the attributes of a complex dataset, like having time-series that somehow capture the actual data‚Äôs statistical properties, we will need a tool that generates data using different approaches. Synthetic Data Vault (SDV) python library is a tool that models complex datasets using statistical and machine learning models. This tool can be a great new tool in the toolbox of anyone who works with data and modeling.\n\n\nThe main reason I‚Äôm interested in this tool is for¬†system testing: It‚Äôs much better to have datasets that are generated from the same actual underlying process. This way we can test our work/model in a realistic scenario rather than having unrealistic cases. There are other reasons why we need synthetic data such as data understanding, data compression, data augmentation, and data privacy [1].\nThe¬†Synthetic Data Vault (SDV)¬†was first introduced in the paper¬†‚ÄúThe Synthetic data vault‚Äú, then used in the context of generative modeling in the master thesis ‚ÄúThe Synthetic Data Vault: Generative Modeling for Relational Databases‚Äù¬†by Neha Patki. Finally, the SDV library was developed as a part of Andrew Montanez‚Äôs master thesis¬†‚ÄúSDV: An Open Source Library for Synthetic Data Generation‚Äú. Another master thesis to add new features to SDV was done by Lei Xu¬†‚ÄúSynthesizing Tabular Data using conditional GAN‚Äú.\nAll these work and research were done in the MIT Data-to-AI laboratory under the supervision of Kalyan Veeramachaneni ‚Äì a principal research scientist at MIT Laboratory for Information and Decision Systems (LIDS, MIT).\nThe reason I‚Äôm bringing the history of the SDV is to appreciate the amount of work and research that has gone behind this library. An interesting article talking about the potential of using this tool, particularly in data privacy is available here."
  },
  {
    "objectID": "blog/sdv-library-for-modeling-datasets/index.html#time-series-data-modeling-using-par",
    "href": "blog/sdv-library-for-modeling-datasets/index.html#time-series-data-modeling-using-par",
    "title": "Essi Alizadeh",
    "section": "Time-Series Data Modeling using PAR",
    "text": "Time-Series Data Modeling using PAR\nA probabilistic autoregressive (PAR) model is used to model multi-type multivariate time-series data. The SDV library has this model implemented in¬†the PAR¬†class¬†(from time-series module).\nLet‚Äôs work out an example to explain different arguments of¬†PAR¬†class. We are going to work with a time-series of temperatures in multiple cities. The dataset will have the following column:¬†Date,¬†City,¬†Measuring Device,¬†Where, Noise.\nIn¬†PAR, there are four types of columns considered in a dataset.\n\nSequence Index: This is the data column with the row dependencies (should be sorted like datetime or numeric values). In time-series, this is usually the time axis. In our example, the sequence index will be the¬†Date¬†column.\nEntity Columns: These columns are the abstract entities that form the group of measurements, where each group is a time-series (hence the rows within each group should be sorted). However, rows of different entities are independent of each other. In our example, the entity column(s) will be only the¬†City¬†column. By the way, we can have more columns as the argument type should be a list.\nContext Columns: These columns provide information about the time-series‚Äô entities and will not change over time. In other words, the¬†context columns should be constant within groups. In our example,¬†Measuring Device¬†and¬†Where¬†are the context columns.\nData Columns: Any other columns that do not belong to the above categories will be considered data columns. The¬†PAR¬†class does not have an argument for assigning data columns. So, the remaining columns that are not listed in any of the previous three categories will automatically be considered data columns. In our example, the¬†Noise¬†column is the data column.\n\n\nSample code\n\n\nExample 1: Single Time-Series (one entity)\nThe PAR model for time series is implemented in¬†PAR()¬†class from¬†sdv.timeseries¬†module. If we want to model a single time-series data, then we only need to set the¬†sequence_index¬†argument of the¬†PAR()¬†class to the datetime column (the column illustrating the order of the time-series sequence).\nimport pandas as pd\nfrom sdv.timeseries import PAR\n\nactual_data = pd.read_csv(\"./daily_min_temperature_data_melbourne.csv\")\nactual_data[\"Date\"] = pd.to_datetime(actual_data[\"Date\"])\n\n# Define -> Fit -> Save a PAR model\nmodel = PAR(sequence_index=\"Date\")\nmodel.fit(actual_data)\nmodel.save(\"data_generation_model_single_city.pkl\")\n\n# After a model is trained and pickled. We can comment above code and just load the model next time. \nmodel = PAR.load(\"data_generation_model_single_city.pkl\")\n\n# Generate new data\nnew_data = model1.sample(num_sequences=1)   # may take few seconds to generate\nnew_data[\"Date\"] = pd.to_datetime(new_data[\"Date\"])\n\n# Compare descriptive statistics\nstat_info1 = actual_data.describe().rename(columns={\"Temp\": \"Real Data\"})\nstat_info2 = new_data.describe().rename(columns={\"Temp\": \"PAR Generated Data\"})\n\nprint(stat_info1.join(stat_info2))\n\nstat_info1.join(stat_info2)\n\n\n\nSynthetic%20Data%20Vault%20(SDV)%20A%20Python%20Library%20for%20Da%204335036dc6a34840ad641aa682aff56a/Untitled%201.png\n\n\n\n\nExample 2: Time-series with multiple entities\nThe SDV is capable of having multiple entities meaning multiple time-series. In our example, we have temperature measurements for multiple cities. In other words, each city has a group of measurements that will be treated independently.\nimport pandas as pd\nfrom sdv.timeseries import PAR\n\nall_data = pd.read_csv(\"./fake_time_series_data_multiple_cities.csv\")\nall_data[\"Date\"] = pd.to_datetime(all_data[\"Date\"])\n\n# Define -> Fit -> Save a PAR model\nmodel = PAR(\n    entity_columns=[\"City\"],\n    context_columns=[\"Measuring Device\", \"Location\"],\n    sequence_index=\"Date\",\n)\nmodel.fit(all_data)\nmodel.save(\"data_generation_model_multiple_city.pkl\")\n\n# After a model is trained and pickled. We can comment above code and just load the model next time. \nmodel = PAR.load(\"data_generation_model_multiple_city.pkl\")\n\n# Generate new data for two fake cities\nnew_data = model1.sample(num_sequences=2)   # may take few seconds to generate\n\n# Compare descriptive statistics\nstat_info1 = all_data.describe().rename(columns={\"Temp\": \"Original Data\"})\nstat_info2 = new_cities[new_cities[\"City\"] == cities[0]].describe().rename(columns={\"Temp\": f\"PAR Generated Data {cities[0]}\"})\nstat_info3 = new_cities[new_cities[\"City\"] == cities[1]].describe().rename(columns={\"Temp\": f\"PAR Generated Data {cities[1]}\"})\nstat_info4 = all_data[all_data[\"City\"] == \"City A\"].describe().rename(columns={\"Temp\": \"Original Data (City A)\"})\nstat_info5 = all_data[all_data[\"City\"] == \"City B\"].describe().rename(columns={\"Temp\": \"Original Data (City B)\"})\nstat_info6 = all_data[all_data[\"City\"] == \"City C\"].describe().rename(columns={\"Temp\": \"Original Data (City C)\"})\n\nstat_comparison = stat_info1.join(stat_info2).join(stat_info3).join(stat_info4).join(stat_info5).join(stat_info6)\nstat_comparison\n\n\n\nSynthetic%20Data%20Vault%20(SDV)%20A%20Python%20Library%20for%20Da%204335036dc6a34840ad641aa682aff56a/Untitled%202.png\n\n\nA detailed example of time-series modeling using the PAR model can be found here."
  },
  {
    "objectID": "blog/sdv-library-for-modeling-datasets/index.html#relational-data",
    "href": "blog/sdv-library-for-modeling-datasets/index.html#relational-data",
    "title": "Essi Alizadeh",
    "section": "Relational Data",
    "text": "Relational Data\nSDV can model relational datasets by generating data after you specify the data schema using¬†sdv.Metadata(). Moreover, you can plot the entity-relationship (ER) diagram by using the library built-in function. After the metadata is ready, new data can be generated using the Hierarchical Modeling Algorithm. You can find more information¬†here."
  },
  {
    "objectID": "blog/sdv-library-for-modeling-datasets/index.html#single-table-data",
    "href": "blog/sdv-library-for-modeling-datasets/index.html#single-table-data",
    "title": "Essi Alizadeh",
    "section": "Single Table Data",
    "text": "Single Table Data\nSDV can also model a single table dataset. It uses statistical and deep learning models that are:\n\nA Gaussian Copula to model the multivariate distribution, and\nA Generative Adversarial Network (GAN) to model tabular data (based on the paper¬†‚ÄúModeling Tabular data using Conditional GAN‚Äú.\n\nMore information about modeling single table datasets is available¬†here."
  },
  {
    "objectID": "blog/sdv-library-for-modeling-datasets/index.html#benchmarking-data",
    "href": "blog/sdv-library-for-modeling-datasets/index.html#benchmarking-data",
    "title": "Essi Alizadeh",
    "section": "Benchmarking Data",
    "text": "Benchmarking Data\nThe SDV library provides the ability to benchmark synthetic data generators using¬†the SDGym library¬†to evaluate the performance of synthesizer. You can find more information¬†here."
  },
  {
    "objectID": "blog/statistics-data-vs-sampling-distribution/index.html",
    "href": "blog/statistics-data-vs-sampling-distribution/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Data Distribution\nMuch of the statistics deals with inferring from samples drawn from a larger population. Hence, we need to distinguish between the analysis done the original data as opposed to analyzing its samples. First, let‚Äôs go over the definition of the data distribution:\n\nüí° Data distribution:¬†The frequency distribution of individual data points in the original dataset.\n\nLet‚Äôs first generate random skewed data that will result in a non-normal (non-Gaussian) data distribution. The reason behind generating non-normal data is to better illustrate the relation between data distribution and the sampling distribution.\nSo, let‚Äôs import the Python plotting packages and generate right-skewed data.\n# Plotting packages and initial setup\nimport seaborn as sns\nsns.set_theme(palette=\"pastel\")\nsns.set_style(\"white\")\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams[\"figure.dpi\"] = 150\n# Generate Right-Skewed data set\nfrom scipy.stats import skewnorm\nfrom sklearn.preprocessing import MinMaxScaler\n\nnum_data_points = 10000\nmax_value = 100\nskewness = 15   # Positive values are right-skewed\n\nskewed_random_data = skewnorm.rvs(a = skewness,loc=max_value, size=num_data_points, random_state=1)  \nskewed_data_scaled = MinMaxScaler().fit_transform(skewed_random_data.reshape(-1, 1))\n\n# Plot the data (population) distribution\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_title(\"Data Distribution\", fontsize=24, fontweight=\"bold\")\n\nsns.histplot(skewed_data_scaled, bins=30, stat=\"density\", kde=True, legend=False, ax=ax)\n\n\n\nThe histogram of generated right-skewed data\n\n\nThe histogram of generated right-skewed data\n\n\nSampling Distribution\nIn the sampling distribution, you draw samples from the dataset and compute a statistic like the mean. It‚Äôs very important to differentiate between the data distribution and the sampling distribution as most confusion comes from the operation done on either the original dataset or its (re)samples.\n\nüí° Sampling distribution: The frequency distribution of a sample statistic (aka metric) over many samples drawn from the dataset[1]. Or to put it simply, the distribution of sample statistics is called the sampling distribution.\n\nThe algorithm to obtain the sampling distribution is as follows:\n\nDraw a sample from the dataset.\nCompute a statistic/metric of the drawn sample in Step 1 and save it.\nRepeat Steps 1 and 2 many times.\nPlot the distribution (histogram) of the computed statistic.\n\nimport numpy as np\nimport random\n\nsample_size = 50\nsample_mean = []\n\nrandom.seed(1) # Setting the seed for reproducibility of the result\nfor _ in range(2000):\n    sample = random.sample(skewed_data_scaled.tolist(), sample_size=50) \n    sample_mean.append(np.mean(sample))\n                    \nprint(f\"Mean: {np.mean(sample_mean)} \\n\")\n\n# Plot the sampling distribution\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_title(\"Sampling Distribution\", fontsize=24, fontweight=\"bold\")\n\nsns.histplot(sample_mean, bins=30, stat=\"density\", kde=True, legend=False)\n>>> Mean: 0.23269\n\n\n\nData%20Distribution%20vs%20Sampling%20Distribution%20What%20Yo%2089ae3957a7154fa098dad2a05926e48f/Untitled%201.png\n\n\nAbove sampling distribution is basically the histogram of the mean of each drawn sample (in above, we draw samples of 50 elements over 2000 iterations). The mean of the above sampling distribution is around 0.23, as can be noted from computing the mean of all samples means.\n\n‚ö†Ô∏è Do not confuse the sampling distribution with the sample distribution. The sampling distribution considers the distribution of sample statistics (e.g.¬†mean), whereas the sample distribution is basically the distribution of the sample taken from the population.\n\n\n\nCentral Limit Theorem (CLT)\n\nüí° Central Limit Theorem: As the sample size gets larger, the sampling distribution tends to be more like a normal distribution (bell-curve shape).\n\nIn CLT, we analyze the sampling distribution and not a data distribution, an important distinction to be made. CLT is popular in hypothesis testing and confidence interval analysis, and it‚Äôs important to be aware of this concept, even though with the use of bootstrap in data science, this theorem is less talked about or considered in the practice of data science[1]. More on bootstrapping is provided later in the post.\n\n\nStandard Error (SE)\nThe standard error is a metric to describe the variability of a statistic in the sampling distribution. We can compute the standard error as follows:\n\n\\text{Standard~Error} = SE = \\frac{s}{\\sqrt{n}}\n\nwhere s denotes the standard deviation of the sample values and n denotes the sample size. It can be seen from the formula that as the sample size increases, the SE decreases.\nWe can estimate the standard error using the following approach[1]:\n\nDraw a new sample from a dataset.\nCompute a statistic/metric (e.g., mean) of the drawn sample in Step 1 and save it.\nRepeat Steps 1 and 2 several times.\nAn estimate of the standard error is obtained by computing the standard deviation of the previous steps‚Äô statistics.\n\nWhile the above approach can be used to estimate the standard error, we can use bootstrapping instead, which is preferable. I will go over that in the next section.\n\n‚ö†Ô∏è Do not confuse the standard error with the standard deviation. The standard deviation captures the variability of the individual data points (how spread the data is), unlike the standard error that captures a sample statistic‚Äôs variability.\n\n\n\nBootstrapping\nBootstrapping is an easy way of estimating the sampling distribution by randomly drawing samples from the population (with replacement) and computing each resample‚Äôs statistic. Bootstrapping does not depend on the CLT or other assumptions on the distribution, and it is the standard way of estimating SE[1].\nLuckily, we can use [bootstrap()](https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap/) functionality from the MLxtend library (You can read my post on MLxtend library covering other interesting functionalities). This function also provides the flexibility to pass a custom sample statistic.\nfrom mlxtend.evaluate import bootstrap\n\navg, std_err, ci_bounds = bootstrap(\n    skewed_data_scaled,\n    num_rounds=1000,\n    func=np.mean,  # A function to compute a sample statistic can be passed here\n    ci=0.95,\n    seed=123 # Setting the seed for reproducibility of the result\n)\n\nprint(\n    f\"Mean: {avg.round(5)} \\n\"\n    f\"Standard Error: +/- {std_err.round(5)} \\n\"\n    f\"CI95: [{ci_bounds[0].round(5)}, {ci_bounds[1].round(5)}]\"\n)\n>>> Mean: 0.23293\n>>> Standard Error: +/- 0.00144\n>>> CI95: [0.23023, 0.23601]\n\n\nConclusion\nThe main takeaway is to differentiate between whatever computation you do on the original dataset or the sample of the dataset. Plotting a histogram of the data will result in data distribution, whereas plotting a sample statistic computed over samples of data will result in a sampling distribution. On a similar note, the standard deviation tells us how the data is spread, whereas the standard error tells us how a sample statistic is spread out.\n\nüëâ You can find the Jupyter notebook for this blog post on GitHub.\n\n\nThanks for reading üôè\nIf you liked this post, you can join my mailing list to receive similar posts. You can follow me on LinkedIn, GitHub, Twitter and Medium.\nAnd finally, you can find my knowledge forest üå≤ (raw digital notes) at notes.ealizadeh.com.\n\n\nüì© Join my mailing list\nhttps://chilipepper.io/form/burning-darkcrimson-piquillo-fcdff002-5e7b-4d46-a75e-6290908a51f4\n\n\n\nReferences\n[1] P. Bruce & A. Bruce (2017),¬†Practical Statistics for Data Scientists, First Edition, O‚ÄôReilly\n\n‚¨ÖÔ∏è Previous Post\nBlog Posts\n‚û°Ô∏è Next Post\nBlog Posts\n\n\n\n\nCitationBibTeX citation:@online{alizadeh,\n  author = {Essi Alizadeh},\n  editor = {},\n  url = {https://ealizadeh.com/blog/statistics-data-vs-sampling-distribution},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi Alizadeh. n.d. https://ealizadeh.com/blog/statistics-data-vs-sampling-distribution."
  },
  {
    "objectID": "blog/working-with-missing-values-in-pandas-and-numpy/index.html",
    "href": "blog/working-with-missing-values-in-pandas-and-numpy/index.html",
    "title": "Essi Alizadeh",
    "section": "",
    "text": "Finding null objects in Pandas & NumPy\nIt is always safer to use NumPy or Pandas built-in methods to check for NAs. In NumPy, we can check for NaN entries by using¬†numpy.isnan()¬†method. NumPy only supports its NaN objects and throws an error if we pass other null objects to numpy.isnan().\nnumpy.isnan(np.nan)\n# >>> True\n\nnumpy.isnan(None)\n# >>> TypeError\nI suggest you use pandas.isna() or its alias pandas.isnull() as they are more versatile than numpy.isnan() and accept other data objects and not only numpy.nan.\n# pandas.isna() is an alias of pandas.isnull()\npandas.isna(np.nan)\n# >>> True\n\npandas.isna(None)\n# >>> True\n\npandas.isna(pd.NaT)\n# >>> True\n\n\nCalculations with missing data\nLet me tell you a story that happened to me a few days ago. I wanted to calculate the Median Absolute Deviation using¬†mad()¬†from the¬†statsmodel¬†library that is dependent on the¬†median()¬†function from NumPy. I had NaN entries in the data I was working on, and consequently, the output result was NaN since there was at least one missing value in the input array. It took me some time to find this semantic error. So, I figured the following out in a hard way:\n\n‚ö†Ô∏è Missing values propagate through arithmetic operations in NumPy and Pandas unless they are dropped or filled with a value.\n\nThe following examples illustrate what happens when we calculate some statistics from our data without considering the missing values:\n2 + numpy.nan\n# >>> nan\n\nnumpy.nan / 2\n# >>> nan\nYou have to be cautious about NaNs in your data when you are calculating any statistic. For example, let‚Äôs calculate the mean of an array including a NaN.\nnumpy.mean([1.0, 2.0, 3.0, numpy.NaN])\n# >>> nan\n\nnumpy.nanmean([1.0, 2.0, 3.0, numpy.NaN])\n# >>> 2.0\nNumPy functions that calculate data statistics usually have counterpart functions to work with NaNs such as numpy.nansum() and numpy.nanstd().\n\n\n\nRecommendations\n\nAlways keep in mind the difference between equality operators ‚Äú==‚Äù and ‚Äúis‚Äù.\nUse¬†Pandas¬†built-in methods to check for NA entries.\nPay attention to the behavior of functions in the presence of null objects, particularly functions to calculate statistical properties.\n\n\n\n\nConclusion\nI believe next time you work with null objects in Python, you pay more attention to them. YI hope you learned something useful from my first ever article on Medium.com. Feel free to provide me with any feedback or suggestion.\n\nüìì You can find a notebook for this article on GitHub that includes additional examples.\n\n\nThanks for reading üôè\nIf you liked this post, you can join my mailing list to receive similar posts. You can follow me on LinkedIn, GitHub, Twitter and Medium.\n\n\nüì© Join my mailing list\nhttps://chilipepper.io/form/burning-darkcrimson-piquillo-fcdff002-5e7b-4d46-a75e-6290908a51f4\n\n\n\nUseful Links\nWorking with missing data - pandas 1.2.3 documentation\nThe Difference Between ‚Äúis‚Äù and ‚Äú==‚Äù in Python - dbader.org\n\n‚û°Ô∏è Next Post\nBlog Posts\n\n\n\n\nCitationBibTeX citation:@online{essi,\n  author = {Essi},\n  editor = {},\n  url = {https://ealizadeh.com/blog/working-with-missing-values-in-pandas-and-numpy},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEssi. n.d. https://ealizadeh.com/blog/working-with-missing-values-in-pandas-and-numpy."
  },
  {
    "objectID": "blog/cognitive-errors-art-of-thinking-clearly/index.html#sec-base-rate-neglect",
    "href": "blog/cognitive-errors-art-of-thinking-clearly/index.html#sec-base-rate-neglect",
    "title": "15 Cognitive Errors Every Analyst Must Know (+ Network Graph View)",
    "section": "1. Base-Rate Neglect",
    "text": "1. Base-Rate Neglect\nThis fallacy is a common reasoning error where people neglect the distribution of the data in favor of specific individual information. Here is an example of this bias from the book.\nMark is a man from Germany who wears glasses and listens to Mozart. Which one is more likely?\nHe is 1) a truck driver or 2) a literature professor in Frankfurt?\nMost people will bet on Option 2 (the wrong option). The number of truck drivers in Germany is 10,000 times more than the number of literature professors in Frankfurt. Hence, it‚Äôs more likely that Mark is a truck driver (see Dobelli 2013).\n\n1.1 False Positive Paradox\nThe false positive paradox is *an example of base-rate bias when the number of false positives is more than the number of true positives1.\nExample: Imagine that 1% of a population is actually infected with a disease, and there is a test with a 5% false-positive rate and no false-negative rate, i.e. False Negative or FN‚ÄÑ=‚ÄÑ0FN=0. The expected outcome of 10000 tests would be\n\nInfected and the test correctly indicates the diseases (True Positive): 10000 \\times \\frac{1}{100} = 100 \\; (TP = 100)\nUninfected and the test incorrectly indicates the person has the disease (False Positive) 10000 \\times \\frac{100 - 1}{100} \\times 0.05 = 495 \\; (FP=495)\n\nSo, a total of 100+495=595 people tested positive. And the remaining 10000‚àí595=9405 \\; (TN=9405) tests have correct negative results (True Negative).\nOverall, only 100 of the 595 positive results are actually correct. The probability of actually being infected when the test results are positive is \\frac{100}{100 + 495} = 0.168 or 16.8\\%, for a test with an accuracy of 95\\% (\\frac{TP + TN}{TP + FP + FN + TN} = \\frac{100 + 9405}{10,000}=0.9505)"
  }
]